<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Performance Engineering Notes</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="01_intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="02_profiling.html"><strong aria-hidden="true">2.</strong> Profiling</a></li><li class="chapter-item expanded "><a href="03_modeling.html"><strong aria-hidden="true">3.</strong> Performance Modeling</a></li><li class="chapter-item expanded "><a href="04_efficiency.html"><strong aria-hidden="true">4.</strong> Writing Efficient Code</a></li><li class="chapter-item expanded "><a href="05_parallelism.html"><strong aria-hidden="true">5.</strong> Multi-Core Systems and Parallelism</a></li><li class="chapter-item expanded "><a href="06_device_performance.html"><strong aria-hidden="true">6.</strong> System and Device Performance</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Performance Engineering Notes</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="lecture-1-introduction"><a class="header" href="#lecture-1-introduction">Lecture 1: Introduction</a></h1>
<p>Systems are made up from <strong>components</strong> that interact to achieve a greater goal. They are usually applicable to different problems and domains, rather than HPC, which focuses on a single problem of high value.</p>
<p><strong>Flexibility is key</strong> in a system: the exact conditions under which a system operates are unknown at development time.</p>
<p>For example:</p>
<ul>
<li>A Data management system does not know the data format/schema beforehand</li>
<li>Grep does not know what regex to search for</li>
</ul>
<p>In addition, software systems need to be <strong>maintainable</strong> because of how complex they are due to many years of development.</p>
<p>Finally, systems should be <strong>fast</strong>: a faster system is generally better than a slower system.</p>
<p>The <strong>challenge is to build a system which is balanced enough where it is maintainable, flexible and fast</strong>.</p>
<h2 id="live-coding-session"><a class="header" href="#live-coding-session">Live coding session</a></h2>
<p>Write a program that counts the lines of a text file containing the phrase <code>  FAUST:</code>.</p>
<p>1 - We first build a functional version:</p>
<pre><code class="language-c++">// Interface
class Operator {
public:
    virtual char const* getNextString() = 0;
};

// Read an array one time
class ArrayReader : public Operator {
private:
    char const* inputArray;
    bool isRead{false};

public:
    ArrayReader(char const* array) : inputArray(array) {}

    char const* getNextString() override {
        if (isRead) {
            return nullptr;
        }
        isRead = true;
        return inputArray;
    }
};

class LineSeparator : public Operator {
private:
    Operator&amp;&amp; input;
    char const* currentString{nullptr};

public:
    LineSeparator(Operator&amp;&amp; input) : input(std::move(input)) {}

    char const* getNextString() override {
        if (currentString == nullptr) {
            currentString = input.getNextString();
        }

        auto result = currentString;

        // Advance until the next string for later calls
        if (currentString != nullptr) {
            while (*currentString != '\n' &amp;&amp; *currentString != '\0') {
                currentString++;
            }

            if (*currentString == '\n') {
                currentString++;
            } else if (*currentString == '\0') {
                return nullptr;
            }
        }

        return result;
    }
};

class LineMatcher : public Operator {
private:
    Operator&amp;&amp; input;
    char const* currentString{nullptr};

public:
    LineMatcher(Operator&amp;&amp; input) : input(std::move(input)) {}

    char const* getNextString() override {
        auto candidate = input.getNextString();
        while (candidate != nullptr) {
            // try to match with our target string
            if (candidate[0] == ' ' &amp;&amp;
                candidate[1] == 'F' &amp;&amp;
                ...) {
                    return candidate;
                }

            candidate = input.getNextString();
        }

        return nullptr;
    }

};

class LineCounter {
private:
    Operator&amp;&amp; input;
public:
    LineCounter(Operator&amp;&amp; input) : input(std::move(input)) {}

    unsigned long getCount() {
        unsigned long count = 0;
        auto line = input.getNextString();
        while (line != nullptr) {
            count++;
            line = input.getNextString();
        }

        return result;
    }
};

int main(int argc, char *argv[]) {
    auto fd = open(argv[1], O_RDONLY);
    auto fsize = lseek(fd, 0, SEEK_END);
    lseek(fd, 0, SEEK_SET);

    auto data = (char*) mmap(nullptr, fsize, PROT_READ, MAP_SHARED, fd, 0);

    auto res = LineCounter(
        LineMatcher(
            LineSeparator(
                ArrayReader(data)
            )
        )
    ).getCount();

    printf(&quot;Count: %lu\n&quot;, res);
    return 0;
}
</code></pre>
<p>2 - If we benchmark our program using <code>time</code> and compare with <code>grep</code>, we see that our program is x4 slower (for large file sizes). Now, we try to run the string matching on the main function directly by comparing the bytes, instead of using all of the classes and OOP-style code.</p>
<p>3 - We should also try to compile with <code>-O3</code> optimisations, which gives us a good performance boost.</p>
<p>4 - We can improve the string matching loop by directly comparing an 8-byte word at a time:</p>
<pre><code class="language-c++">// Get the bit pattern of what we're looking for
long word = *((long *) &quot;  FAUST:&quot;);

for (auto i = 0; i &lt; fsize - 9; i++) {
    long currWord = *((long*)(data + i);
    if (currWord == word &amp;&amp; data[i+8] == '\n') {
        count++;
    }
}
</code></pre>
<p>This gives us a x2 speed increase compared to <code>grep</code>. <strong>This is what performance engineering is about</strong>: we first meet the functional requirements and then optimise for performance.</p>
<p><strong>Performance engineering</strong> is about the techniques applied during a systems development life cycle to ensure the non-functional requirements for performance will be met.</p>
<h2 id="when-to-stop-optimising"><a class="header" href="#when-to-stop-optimising">When to stop optimising?</a></h2>
<p>How do we know when to stop optimising? How fast is <em>fast enough</em>?</p>
<ol>
<li>
<p><strong>Define a target metric</strong>, for example:</p>
<ul>
<li>throughput (ops/s)</li>
<li>latency (s)</li>
<li>memory usage</li>
<li>scalability</li>
<li>etc.</li>
</ul>
</li>
<li>
<p><strong>Decide when the requirements are met</strong>, two options:</p>
<ul>
<li>Set an optimisation budget (in terms of developer time and/or salary)</li>
<li>Set an optimisation target/requirement (QoS), such as <em>real-time requirements</em>:
<ul>
<li><strong>Soft real-time-req</strong>: if missed, the software is <em>an error</em></li>
<li><strong>Hard real-time-req</strong>: if missed, the software is <em>a failure</em></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The latter type of requirement is known as a <strong>Quality-of-Service (QoS)</strong> objective, which is a statistical properties of a metric that shall hold for the system. For example:</p>
<blockquote>
<p>&quot;<em>The framerate of the game will, on average, be higher than 60 frames per second if run on a GPU with 50 GFlops or more.</em>&quot;</p>
</blockquote>
<p>QoS requirements can sometime conflict with functional requirements, so it is important to find the right balance.</p>
<p>A <strong>Service-Level Agreement (SLA)</strong> is a formal, legal contract specifying QoS objectives as well as penalties for violations of such objectives. For example:</p>
<blockquote>
<p>&quot;<em>Trading orders shall not exceed 1ms response time. In case of violation, the user is eligible for a 10% credit towards fees.</em>&quot;</p>
</blockquote>
<p>When defining SLAs and QoS objectives, you should be <em>SMART</em>:</p>
<ul>
<li>Specific: state exactly what is acceptable in numeric terms</li>
<li>Measurable: make sure what is stated can actually be measured</li>
<li>Acceptable: rigorous enough to guarantee success in reality</li>
<li>Realizable: lenient enough to allow implementation</li>
<li>Thorough: ensure that all necessary aspect of the system are specified</li>
</ul>
<h2 id="measuring-and-performance-evaluation"><a class="header" href="#measuring-and-performance-evaluation">Measuring and performance evaluation</a></h2>
<p>In this course, we will focus on <strong>measurability</strong> but there are different techniques:</p>
<ul>
<li><strong>Measuring</strong>:
<ul>
<li>Monitoring</li>
<li>Benchmarking</li>
</ul>
</li>
<li>Analytical Modeling</li>
<li>Simulation</li>
<li>Hybrid techniques:
<ul>
<li>Model, then simulate, then measure</li>
<li>etc.</li>
</ul>
</li>
</ul>
<p>Measuring is performed on the actual system, whether it is on a prototype or the final system. Measuring is often based on <em>instrumentation</em> and can achieve good accuracy, but it is often costly and can be difficult.</p>
<h3 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h3>
<p>Monitoring refers to <strong>measuring in production</strong>. Monitoring is actually <strong>required to enforce SLAs</strong> in order to:</p>
<ul>
<li>Observe system performance,</li>
<li>Collect statistics,</li>
<li>Analyze data,</li>
<li>Report SLA violations</li>
</ul>
<p>Monitoring can incur cost, as you may need to setup monitoring systems and can directly affect the performance of the workload. Hence, monitoring is often <strong>not continuous</strong>.</p>
<h3 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h3>
<p>Benchmarking refers to &quot;<em>measuring in the lab</em>&quot;. It is a two-step process:</p>
<ol>
<li>Get the system into a predefined (or steady) state.</li>
<li>Perform a series of operations (the workload) while measuring relevant performance metric.</li>
</ol>
<p>For example, database workloads usually have a data generator to make the system load a dataset (i.e., predefined state) and a query set (the series of operations).</p>
<p>The benchmark workload should be representative of production workloads but smaller for easier benchmarking. They can be:</p>
<ul>
<li>
<p><strong>Batch workload</strong>: the program has access to the entire batch from the start, useful when the metric is <strong>throughput</strong>. Here, the work generator is unimportant.</p>
</li>
<li>
<p><strong>Interactive workload</strong>: work is generated randomly one at-a-time, useful when the metric is <strong>latency</strong>. The work generator should be as least as fast as the system, otherwise the system becomes under-utilised during the benchmark.</p>
</li>
</ul>
<h2 id="interpreting-results"><a class="header" href="#interpreting-results">Interpreting results</a></h2>
<p>A single measured datapoint is meaningless, due to the large amount of noise in modern computer systems. This is basic statistics: we need to aggregate multiple runs of the benchmark and report a measure of variance.</p>
<h2 id="optimisation-loop"><a class="header" href="#optimisation-loop">Optimisation loop</a></h2>
<img src="assets/01_optimisation_loop.png">
<p>A key part of this lifecycle is to <strong>identify optimisation opportunities</strong>.</p>
<p>A <strong>parameter</strong> is a system and workload characteristics that affect performance. This is purposely a broad definition, so we can categorise parameters into two types:</p>
<ul>
<li>
<p><strong>System Parameters</strong>: those that generally do not change while the system runs (caches, CPU instruction costs, ...)</p>
</li>
<li>
<p><strong>Workload Parameters</strong>: those that may change, even while the system is running (users, available memory, other processes...)</p>
</li>
</ul>
<p>We can also divide parameters based on the representation: <em>numerical parameters</em> (such as CPU frequency) or <em>nominal/categorical parameters</em> (such as the target device class).</p>
<p><strong>Utilisation</strong> is the percentage of a resource that is used to perform the service. Any service will have a certain amount of resources available (eg CPU cycles, memory capacity, network bandwidth). As such, the utilisation is a numeric parameter.</p>
<h3 id="bottlenecks"><a class="header" href="#bottlenecks">Bottlenecks</a></h3>
<p>Given this definition, a <strong>bottleneck</strong> is the resource with the highest utilisation. We use the term <em>x-bound</em> to indicate that <em>x</em> is the bottleneck of an application:</p>
<ul>
<li>CPU-bound -&gt; faster CPU needed</li>
<li>Disk/network bandwidth-bound -&gt; more disk/network bandwidth needed</li>
<li>etc.</li>
</ul>
<p>Unfortunately, <strong>identifying bottlenecks for an entire complex software system is practically infeasible</strong>. Instead, we should look at the <strong>performance-dominating</strong> code paths and dedicate our effort to optimising such paths.</p>
<p>Some paths have special names:</p>
<ul>
<li>
<p><strong>Critical path</strong>: the sequential part of the code in a parallel system, ie: the longest piece of sequential code needed to perform a particular service.</p>
</li>
<li>
<p><strong>Hot path</strong>: the path that takes the most time.</p>
</li>
</ul>
<h2 id="how-to-optimise"><a class="header" href="#how-to-optimise">How to optimise?</a></h2>
<p>The goal in optimisation is to:</p>
<ul>
<li>
<p>Compare alternative designs (development)</p>
</li>
<li>
<p>Select a close-to-optimal value for a platform parameter (tuning)</p>
</li>
</ul>
<p>The second strategy is known as <strong>parameter tuning</strong>, which is finding the vector in the parameter space that either:</p>
<ul>
<li>minimises resource consumption, or</li>
<li>maximises a performance metric</li>
</ul>
<p>This implies exploring the parameter space and can be quite expensive. We can build <em>analytical models</em> to accelerate this tuning process.</p>
<h3 id="analytical-performance-models"><a class="header" href="#analytical-performance-models">Analytical performance models</a></h3>
<p>An analytical performance model is a formal characterisation of the relationship between system parameters (hardware, software, data) and performance metrics.</p>
<p>The challenge is to model a dynamic system using a (reasonably small) static model.</p>
<p>Models can be stateless (eg., characterising equations, such as regression curves) or stateful (eg., markov chains), which are usually harder to implement but more accuarate.</p>
<p>Analytical models:</p>
<ul>
<li>are fast</li>
<li>they allow what-if-analysis (to estimate the change of a system/workload parameter)</li>
</ul>
<p>Models are key to defining the achievable performance requirements in an SLA.</p>
<p>Examples of analytical performance models are:</p>
<blockquote>
<p>Example 1: <em>An I/O bound application, that needs to read 40MB per request is limited to 10 requests per second when running on a hardware platform with a single disk providing 400MB/s bandwidth.</em></p>
</blockquote>
<blockquote>
<p>Example 2: <em>A compute-bound application that needs 3 cycles to process one byte is limited to 20 requests per second if it needs to process 40MB per request and the CPU runs at 2.4GHz.</em></p>
</blockquote>
<h3 id="simulation"><a class="header" href="#simulation">Simulation</a></h3>
<p>A simulation is a single observed run of a stateful model.</p>
<p>They can be extremely expensive to calculate especially if the level of detail is high.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-2-profiling-and-performance-tracing"><a class="header" href="#lecture-2-profiling-and-performance-tracing">Lecture 2: Profiling and Performance Tracing</a></h1>
<p>This lecture goes into more detail about identifying optimisation opportunities, specifically, we identify:</p>
<ol>
<li>the hot path (the code that takes the most time)</li>
<li>the bottleneck within the hot path</li>
</ol>
<p>Both of these are functions of <strong>system behaviour</strong>, described by events.</p>
<h2 id="events"><a class="header" href="#events">Events</a></h2>
<p><strong>Events</strong> are any change of the system state. Usually, we restrict events to a certain granularity, for example:</p>
<ul>
<li><strong>Simple/atomic events</strong> (executed an instruction, loaded an address, clock tick, etc.)</li>
<li><strong>Complex events</strong> (cache line evicted from L1 to L2 cache, instruction aborted due to misspeculation, etc.)</li>
</ul>
<p>An event may have an optional payload (which is metadata describing the event in more detail) and an accuracy: the event <strong>accuracy</strong> is the degree to which its value represents reality.</p>
<p>Events originate from an <strong>event source</strong>, which generally has two components:</p>
<ul>
<li>
<p>the <em>generator</em>, which generates a new occurrance of an event. This is usually online (during runtime or part of the system). For example, the code in the kernel that sends network packets can be considered a generator of network events.</p>
</li>
<li>
<p>the <em>consumer</em>, which processes the events. It can be offline (the events are not used for any meaningful purpose) or online (the events are used in some way, such as calculating statistics of the events).</p>
</li>
</ul>
<p>These events can be used for <strong>tracing</strong> or <strong>profiling</strong>.</p>
<h2 id="tracing"><a class="header" href="#tracing">Tracing</a></h2>
<p>A <strong>trace</strong> is a complete log of every state the system has ever been in during a given period of time. Thus, a trace is comprised by a sequence of ordered events.</p>
<p>The accuracy of a trace is <em>inherited</em> from the events. Importantly, <strong>event collection may have a high overhead</strong> during tracing, which can lead to problems.</p>
<h3 id="example-call-stack-tracing"><a class="header" href="#example-call-stack-tracing"><em>Example: Call stack tracing</em></a></h3>
<p>Call stack tracing consists of sampling snapshots of the call stack of the application at regular intervals. Every time a function calls another function, a new frame is added to the stack. Therefore, we can trace how long our program spends in each function call.</p>
<p>A call stack consists of different function frames, each of which contain the required and relevant state to execute the function: variables, the return address and the saved frame pointer (if not optimised away during compilation). Each frame's saved EBP (frame pointer) points to the function it was called from. Hence, we can <em>walk</em> through the stack by following the frame pointers.</p>
<p>However, walking the stack can be <strong>expensive</strong> if the call stack is very deep (as the complexity is linear in terms of the number of function calls). In particular, for small functions, call stack processing can be more expensive than the function itself (because we need to save the EBP).</p>
<h3 id="perturbation-and-sampling"><a class="header" href="#perturbation-and-sampling">Perturbation and sampling</a></h3>
<p>This issue is known as <strong>perturbation</strong>: the degree to which the performance of a system changes when it is being analysed. Although the overhead is unimportant (because we are not going to trace the program during production), perturbation can <strong>negatively affect accuracy</strong> if it is non-deterministic (ie: you don't know how much overhead you are adding).</p>
<p>We can reduce perturbation by reducing <strong>fidelity</strong> (the degree of exactness). In our context, this means:</p>
<ul>
<li>Perfect fidelity means every event is recorded</li>
<li>Reduced fidelity means not every event is recorded </li>
</ul>
<p>In practice, reducing fidelity works via <strong>sampling</strong>, where we collect events in regular intervals.</p>
<p>An <strong>interval</strong> is the distance between two samples being taken. We can specify interval lengths via:</p>
<ul>
<li>
<p><strong>Time-based intervals</strong>: set a hardware recurrent timer and sample whenever it runs out. The notion of time is usually captured via CPU reference cycles (because time in a CPU is not accurate, as clock rates may vary across different cores). These intervals are easy to interpret, as we are measuring time and this is inversely proportional to performance.</p>
</li>
<li>
<p><strong>Event-based intervals</strong>: defined in terms of the occurrence of events (e.g.: sample every fifth function call). This gives us accurate results with low noise (using hardware counters), however it can be tricky to interpret (as we are usually interested in time). </p>
</li>
</ul>
<p>Interval resolution is limited to (usually) discrete clock cycles, however time is continuous. This introduces a quantisation error when attributing an event to a particular clock cycle. For example, if we have an expensive SIMD instruction just before a simple MOV instruction, a profiler may tell us that the <code>MOV</code> instruction is being slow when in reality its the previous instruction which is slow (this is quantisation error).</p>
<p>Returning to the example of call stack sampling, we simply skip some events. This means some functions may be skipped but there is a good chance that expensive functions will be sampled more often. This gives us good performance and reduced perturbation.</p>
<h3 id="indirect-tracing"><a class="header" href="#indirect-tracing">Indirect tracing</a></h3>
<p>An example of event-based interval sampling is <strong>indirect tracing</strong>. The idea is that events dominate each other, so we should only trace the dominant events.</p>
<p>We can think of it as intervals defined by the execution flow: if we have an <code>if</code> statement, then all the code in the branch that is taken will be deterministically executed. Therefore, we do not need to trace every instruction in the branch, we can just record the fact that the branch was taken in the first place. Hence, we say that <strong>control-flow instructions dominate non-control-flow instructions</strong>. We can count how many times a branch is executed (known as basic block counting).</p>
<p>Indirect tracing can be used to reduce overhead/perturbation (because we don't have to sample every instruction), however the fidelity/accuracy depends on the event and how much indirection there is.</p>
<h2 id="profiling"><a class="header" href="#profiling">Profiling</a></h2>
<p>Profiling (in the context of performance engineering) is a characterisation of a system in terms of the resources it spends in the certain states. Since an event is a transition of states, we can derive a profile from events.</p>
<p>Specifically, a profile is an aggregate over the events of a specific metric. This is can be a global aggregate (eg: total cache misses, total CPU cycles) or an event-based aggregate (eg: cycles per instruction, cache misses by line of code).</p>
<p>Profiling is useful for:</p>
<ul>
<li>post-mortem analysis for ease of interpretation</li>
<li>reducing perturbation by aggregating traces into profiles in real-time (assuming aggregation is faster than writing the traces to memory)</li>
</ul>
<p>As mentioned earlier, events originate from <strong>event sources</strong>. These should be detailed, accurate and have little perturbation. Events can originate from:</p>
<ul>
<li>Software:
<ul>
<li>Library: manual instrumentation, logging</li>
<li>Compiler: automatic instrumentation</li>
<li>OS: kernel counters</li>
</ul>
</li>
<li>Hardware:
<ul>
<li>Performance counters</li>
</ul>
</li>
<li>Emulator: a hybrid of both, with minimal perturbation (but not scalable)</li>
</ul>
<h3 id="instrumentation"><a class="header" href="#instrumentation">Instrumentation</a></h3>
<p>Consists of augmenting the program with event logging code. There is no need for hardware support and extremely flexible. However, there is high overhead and high perturbation.</p>
<p>There are three approaches to instrumentation:</p>
<ul>
<li>
<p><strong>Manual instrumentation</strong>: basically using logging using <code>printf</code> (or a logging library).</p>
<ul>
<li>Advantages: fine-grained control, no hardware involvment</li>
<li>Disadvantages: high runtime and implementation overhead</li>
</ul>
</li>
<li>
<p><strong>Automatic source-level instrumentation</strong>: source-to-source rewriting, usually compiler-supported.</p>
<ul>
<li>Advantages: <strong>todo, see interactive lecture</strong></li>
<li>Disadvantages: less control, compiler support required</li>
</ul>
</li>
<li>
<p><strong>Automatic binary instrumentation</strong>:</p>
<ul>
<li>Static (compile-time): simple, portable, instrumentation overhead is easily assessed from the binary</li>
<li>Dynamic (runtime): no recompilation, works on a running process and with JIT-compiled code</li>
</ul>
</li>
</ul>
<p>An example of automatic instrumentation is the LLVM XRay framework, which can be used to automatically log every function call and exit. One limitation of this framework is that it only retains function calls that take more than 5 microseconds. For higher fidelity and lower overhead, we can look at performance counters.</p>
<h2 id="performance-counters"><a class="header" href="#performance-counters">Performance counters</a></h2>
<p>An alternative to instrumentation for profiling is analysing <strong>performance counters</strong>.</p>
<p>Although software counters exist in OS kernels (packets sent/received, virtual memory operations, etc.), they are not often used. Instead, we prefer <strong>hardware performance counters</strong>.</p>
<p>Hardware performance counters are special registers that can be configured to count <em>low-level events</em>. Only a fixed number can be active during the profiling process and can be used to collect events and/or intervals.</p>
<p>Unfortunately, these are often buggy, poorly documented and may be inaccurate. However, the common performance counters are usually fine.</p>
<p>In Linux, we can use the <code>perf</code> tool to access different hardware performance counters. Many of these are often associated to cache behaviour, such as cache line evictions or cache misses.</p>
<h2 id="case-study-microarchitectural-bottleneck-analysis"><a class="header" href="#case-study-microarchitectural-bottleneck-analysis"><em>Case Study: Microarchitectural bottleneck analysis</em></a></h2>
<p>One of the most important types of analyses you can carry out to increase performance of software is by analysing behaviour and identifying bottlenecks at the microarchitectural level.</p>
<h3 id="1-cpu-pipelining-and-frontend-stalls"><a class="header" href="#1-cpu-pipelining-and-frontend-stalls">1. CPU pipelining and frontend stalls</a></h3>
<p>First, we should understand how <strong>CPU pipelining</strong> works. A CPU maintains a pipeline of instructions which are at different stages: fetch, decode, execute, memory and write-back. The ideal situation is that the pipeline is filled, so as many instructions are being processed at a time (instruction-level parallelism). At every cycle, an instruction is <em>retired</em> (removed) from the pipeline.</p>
<p>Unfortunately, having a full pipeline isn't always possible due to <strong>control hazards</strong>, ie: jump instructions, because we need to change the instruction pointer. We cannot know where to jump to until the JMP instruction reaches the last slot of the 5-stage pipeline, meaning we only retire 1 instruction from 5 cycles (80% less than the ideal case). This scenario is known as a <strong>frontend stall</strong>, because the front of the pipeline doesn't know what instruction to read yet.</p>
<p><strong>Branch prediction</strong> is a technique where the CPU speculates upon which branch may be taken and starts loading and executing instructions from within that branch immediately, rather than stalling the pipelining.</p>
<p>When the comparison/branch instructions are finally retired, the CPU figures out if we have correctly predicted the branch. If it mispredicts the branch, the CPU has to flush/clear the pipeline and restart from the branch instruction, which negatively affects performance. Such discarded instructions are referred to <em>abandoned instructions</em>.</p>
<h3 id="2-resource-stalls"><a class="header" href="#2-resource-stalls">2. Resource stalls</a></h3>
<p>An <strong>ALU stall</strong> refers to when an expensive arithmetic instruction takes several clock cycles in the execution/memory stage and blocks the stream of instructions through the pipeline. For example, division is often an ALU stall because it may take several cycles to execute.</p>
<h3 id="3-memory-subsystem-and-data-stalls"><a class="header" href="#3-memory-subsystem-and-data-stalls">3. Memory subsystem and data stalls</a></h3>
<p>Depending on what memory we access (L1 cache, L2 cache, LL cache or main memory), we will get different latencies.</p>
<p>The graph below shows that if we access values in nearby memory locations, the cost (in CPU cycles) is much lower. The greater the access stride, the more costly it is to read memory. This is due to how caches work and the size of cache lines (every 64 bytes corresponds to a different cache line).</p>
<img src="assets/02_memory_access_latency_locality.png">
<p>The next graphs shows memory access latency (in cycles) depending on the size of the data. For a 4kB array, all our memory access will be hitting the L1 cache. Between 32kB and ~4MB, we access the last-level (L3) cache.</p>
<img src="assets/02_memory_access_latency_size.png">
<p>The key takeaway here is that CPUs can also stall on memory accesses.</p>
<h3 id="bottleneck-analysis"><a class="header" href="#bottleneck-analysis">Bottleneck analysis</a></h3>
<p>Given the different types of stalls, we can identify different microarchitectural bottlenecks in order through which they may arise in the pipeline:</p>
<ol>
<li>Memory access stalls (which fetching the operands for an instruction)</li>
<li>Resource allocation stalls (ALU stalls)</li>
<li>Branch mispredictions (leading to abandonded instructions due to flushing)</li>
<li>Control-flow dependencies</li>
</ol>
<p>We can use the following decision tree to determine what the bottleneck is. Our starting node asks if a micro-operation has been issued (ie: has something happened?, has our pipeline advanced?).</p>
<img src="assets/02_microarch_bottleneck_analysis.png">
<p>If nothing has happened and there is no resource allocation stall (such as an ALU stall), then this is a frontend-bound issue, ie: no instructions entered the pipeline, likely due to a control-flow dependency (too many function calls or jumps in memory).</p>
<p>Otherwise, the bottleneck is backend-bound: cache miss stalls or resource stalls.</p>
<p>If there was a micro-op issued and some progress was made in the pipeline, then the instruction can either retire (good case) or abandoned due to bad speculation (likely due to too many branching code).</p>
<br>
<br>
<br>
<h2 id="tutorial-notes"><a class="header" href="#tutorial-notes">Tutorial notes</a></h2>
<p><strong>Possible exam questions</strong>: given an algorithm/data structure, what would the profile look like for it?</p>
<p>Ie: profile of a rebalancing operation in a Red-Black Tree (considering the size, depth, etc.)</p>
<ul>
<li>
<p>A lot of code/instructions (long algorithm, a lot of recursion) -&gt; likely to be frontend bound</p>
</li>
<li>
<p>Random memory accesses -&gt; memory-bound</p>
</li>
<li>
<p>Code is highly OOP with a nice API (many function calls) -&gt; frontend bound</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-3-performance-modeling"><a class="header" href="#lecture-3-performance-modeling">Lecture 3: Performance Modeling</a></h1>
<p>This lecture is about coming up with analytical models for measuring performance. The primary reason for modeling is when we want to learn about a system's performance &quot;on the cheap&quot;, ie: without running the software. </p>
<p>This is useful for when we may need to predict how much we may be charged for the execution of software (cloud services), 
to know how many resources we need to provision a system, etc.</p>
<p><strong>Assumptions:</strong></p>
<ol>
<li>
<p>The input data follows a known distribution (usually uniform without correlation).</p>
</li>
<li>
<p>We ignore system noise (caused by scheduling, external factors, etc.).</p>
</li>
<li>
<p>We will model single-threaded deterministic code.</p>
</li>
</ol>
<p>Under these assumptions, there are two approaches to modeling:</p>
<ul>
<li>
<p><strong>Numerical/Experimental Model</strong>: we run the system, acquire a series of datapoints and come up with a predictive model.</p>
</li>
<li>
<p><strong>Analytical Model</strong>: a formal characterisation of relationship between parameters and performance metrics, often expressed as an equation. </p>
</li>
</ul>
<h2 id="numerical-models"><a class="header" href="#numerical-models">Numerical Models</a></h2>
<p>The first step is to <strong>gather data</strong>. We do this through <em>microbenchmarks</em>, a small specially designed program used to test to performance behaviour of that small portion of the system.</p>
<p>Throughout the lecture we will consider the following microbenchmark which tests the memory subsystem access performance:</p>
<pre><code class="language-c">extern int* input;
extern size_t N;      // some large constant
extern size_t stride; // the parameter of our experiment

int sum = 0;
for (size_t i = 0; i &lt; N; i += stride) {
  sum += input[stride];
}
</code></pre>
<p>The next step is to <strong>interpret the results</strong> of the microbenchmark. A common interpretation technique is interpolation, where we draw a graph based on the datapoints we collected. It is not usually very accurate, but it is easy for humans to understand.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Easy to get (if the system is available to run)</li>
<li>Based on ground truth (this is actual measured behaviour)</li>
<li>Easy to interpret</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>They generalise poorly (cannot easily be applied to new environments, such as more CPUs, more memory, etc.)</li>
<li>Massive amounts of experimental data needed for high-dimensinal parameter spaces</li>
<li>Limited accuracy and prediction confidence</li>
<li>Limited interpretability (contributing factors are implicit)</li>
</ul>
<h2 id="analytical-models"><a class="header" href="#analytical-models">Analytical Models</a></h2>
<p>Analytical modeling requires detailed understanding of the system (the parameters and their effects) and extensive validation.</p>
<p>Model fitting helps you convert empirical/numerical models into analytical models. One way to do this is through regression, which gives you an analytical linear equation describing the data.</p>
<p>Note that the boundary is blurry: <em>interpolation is numerical while regression is analytical</em> can be argued. The key difference is that <strong>analytical models are more expressive</strong>.</p>
<p>To obtain an analytical model, we need two components:</p>
<ol>
<li>
<p><strong>Characteristic equation</strong>: an equation that describes the behaviour of the target metric of your experiment in dependence of a varied parameter (eg: <code>stride</code>).</p>
</li>
<li>
<p><strong>Values for system parameters</strong> (eg: access latency, cache capacity...)</p>
</li>
</ol>
<h3 id="example-1-building-an-analytical-model-for-memory-access-by-stride"><a class="header" href="#example-1-building-an-analytical-model-for-memory-access-by-stride">Example 1: Building an analytical model for memory access by stride</a></h3>
<p>First, we need to understand the system we are trying to model. Below is a diagram of the memory subsystem:</p>
<img src="assets/03_memory_subsystem.png" width="500px">
<p>As such, we need to define our system parameters. For each level in the hierarchy, we have: a block size, the access latency and the capacity.</p>
<img src="assets/03_system_parameters.png" width="450px">
<p>Now, we can develop a characteristic equation for the access stride experiment:</p>
<p>Let \( T_{mem} \) be the average time for a memory access and let \(s\) be the stride size in bytes.</p>
<p>\(T_{mem} = l_0 \cdot min(1, \frac{s}{B_0}) + l_1 \cdot min(1, \frac{s}{B_1}) + l_2 \cdot min(1, \frac{s}{B_2}) + l_3 \cdot min(1, \frac{s}{B_3})\)</p>
<p>Here, each term represents the latency to access layer \(n\) multiplied by the probability of missing layer \(n\) and going to the next layer. Naturally, the larger \(B_n\) is, there is a lower chance for missing at layer \(n\).</p>
<h3 id="example-2-building-an-analytical-model-for-random-memory-access"><a class="header" href="#example-2-building-an-analytical-model-for-random-memory-access">Example 2: Building an analytical model for random memory access</a></h3>
<p>Let's model this benchmark, where we have random access into the array <code>input2</code>:</p>
<pre><code class="language-c">extern int* input1;   // uniform random data
extern int* input2;   // random data

int sum = 0;
for (size_t i = 0; i &lt; inputSize; i++) {
  sum += input2[input1[i]];
}
</code></pre>
<p>First, let us define parameters in terms of memory regions and access patterns (see section 3 of <a href="http://www.vldb.org/conf/2002/S06P03.pdf">this paper</a>).</p>
<ul>
<li>The length of a memory region is \((R.n)\) (ie: the number of stored &quot;elements&quot;)</li>
<li>The width of a memory region is \((R.w)\) (ie: the size of the tuple/struct in words)</li>
<li>The total size of the region \(||R||\) (ie: \(n \cdot w\))</li>
<li>The access pattern is \(u\) (ie: the distance between two accesses)</li>
</ul>
<p>In addition, we can model two types of access patterns:</p>
<ul>
<li>\(\text{Pattern 1} \oplus \text{Pattern 2}\) is the sequential execution of both access patterns.</li>
<li>\(\text{Pattern 1} \odot \text{Pattern 2}\) is the interleaved execution of both access patterns (ie: doing two access in each iteration of a loop)</li>
</ul>
<p>If we suppose that <code>input1</code> contains 1024 uniform random ints and <code>input2</code> has 64 random ints, we can describe the access pattern in terms of an interleaved sequential traversal (into <code>input1</code>) and random access (into <code>input2</code>):</p>
<p>\(s_{trav}(R.n = 1024, R.w = 1, u = 1) \odot rr_{acc}(R.n = 64, R.w = 1, u = 1, r = 1024)\)</p>
<p>Here \(r\) represents the number of accesses. We have \(R.w = 1\) because we are storing integers (we assume integers are a word in size) and \(u = 1\) because our stride is 1 word.</p>
<p>\(s_{trav}\) represents the sequential traversal while \(rr_{acc}\) represents the repetitive random access.</p>
<p>We can do more interesting things, where <code>input1</code> is struct:</p>
<pre><code class="language-c">struct record {int a; int b; int c;};
extern record* input1;    // uniform random data, 1024 values
extern int* input2;       // random data, 64 values

int sum = 0;
for (size_t i = 0; i &lt; inputSize; i++) {
  sum += input2[input1[i].a];
}
</code></pre>
<p>The access pattern description is:</p>
<p>\(s_{trav}(R.n = 1024, R.w = 3, u = 1) \odot rr_{acc}( R.n = 64, R.w = 1, u = 1, r = 1024)\)</p>
<p>From the paper, we support three different access patterns:</p>
<ul>
<li>Sequential traversal <em>\(s_{trav}\)</em>: a sequential sweep over all the values in R exactly once.</li>
<li>Single random traversal <em>\(r_{trav}\)</em>: all elements are accessed exactly once but in a random order.</li>
<li>Repetitive random access <em>\(rr_{acc}\)</em>: randomly accesses \(r\) values (may be hit more than once and some may not even be hit).</li>
</ul>
<br>
<br>
<h2 id="modeling-statefuldynamic-systems"><a class="header" href="#modeling-statefuldynamic-systems">Modeling stateful/dynamic systems</a></h2>
<p>Some components can have a dynamic state, where each state can influence behaviour and performance. The analytical models we have seen so far are stateless.</p>
<p>We can use <strong>stochastical methods</strong> to model stateful systems. We will focus on <strong>Discrete Markov Chains</strong>.</p>
<p>A discrete Markov chain is basically a finite-state machine with transition probabilities. They have a property whereby the next state is only dependent on the previous state and a random variable.</p>
<h3 id="example-modeling-branch-misprediction"><a class="header" href="#example-modeling-branch-misprediction">Example: Modeling branch misprediction</a></h3>
<p>Consider the following microbenchmark:</p>
<pre><code class="language-c">extern int* input;    // uniform random ints between 0 and 100

int sum = 0;
for (size_t i = 0; i &lt; inputSize; i++) {
  if (input[i] &gt; threshold) {
    sum += input[i];
  }
}
</code></pre>
<p>One of the factors contributing to the performance of this code is the <strong>branch misprediction rate</strong>. If <code>threshold</code> is set to 50, the branch predictor will be wrong more often than if it was set to &lt;20 or &gt;80 for example (because the data is random).</p>
<p>We can model branch misprediction rate using a Markov chain:</p>
<img src="assets/03_markov_branch_predictor.png">
<p>The branch predictor starts in any of the shown states, which represents the <em>confidence</em> of the branch predictor based on whether the branch is actually taken or not.</p>
<p>We can calculate the probability of it being in any state as the <strong>stationary distribution</strong>.</p>
<p>The branch misprediction rate is then given by:</p>
<p>\(( P(pred_taken) \cdot P(act_not_taken) ) + ( P(pred_not_taken) \cdot P(act_taken) )\)</p>
<p>As you can see from the expression above, the worst case scenario is when we are in a <em>not taken state</em> but the branch is actually taken, or when we are in a <em>taken state</em> but the branch is not actually taken.</p>
<p>We can model branch predictors with <strong>more states</strong> if we want to keep a deeper history of past branches and potentially get more accurate branch predictions.</p>
<p>Modeling can be useful because you can verify results that have been stated and you take for granted.</p>
<br>
<br>
<br>
<h2 id="tutorial-exercise-notes"><a class="header" href="#tutorial-exercise-notes">Tutorial exercise notes</a></h2>
<p>Attach to process, run the query a few times and then select the section where there is high activity.</p>
<p>We get that <code>densescan_int</code> is the hotspot function (line 680 <code>scan_sel(densescan, _dense)</code> which is a macro).
We can expand the macro by running the command in section 3.3 (fetches the source file <code>gdk_select.c</code>, puts it through a preprocessor and expands all macros).</p>
<p>Running VTune now with the macro unfolded, we see the actual code which is a <code>for</code> loop and the difference <em>parameters</em> that affect performance.</p>
<p>The behaviour is that we have a large array and we apply some predicate to it. If the predicate holds for the value, we record the index to some output data structure.</p>
<p>We can now write a microbenchmark:</p>
<pre><code class="language-c++">long long inputSize = 128 * 1024 * 1024;

static void selection(benchmark::State&amp; state) {
  auto value = state.range(0);
  auto input = new int[inputSize];

  std::default_random_engine generator;
  std::uniform_int_distribution&lt;int&gt; distribution(1, 501);
  auto getValue = std::bind(distribution, generator);

  for (size_t i = 0; i &lt; inputSize; i++)
    input[i] = getValue();

  auto output = new int[inputSize];
  size_t j = 0;
  for (auto _ : state) {
    for (size_t i = 0; i &lt; inputSize; i++) {
      if (input[i] &lt; value) { // check predicate with input argument
        output[j++] = i;      // write the index if true
      }
    }
    benchmark::DoNotOptimize(ouput);  // important!
  }
  benchmark::DoNotOptimize(input);
  delete[] input;
}

BENCHMARK(selection)
  -&gt;Arg(0*50)
  -&gt;Arg(1*50)
  -&gt;Arg(2*50)
  ...
  -&gt;Arg(10*50)

</code></pre>
<p>This code snippet suffers from <strong>branch misprediction</strong> (the inverse parabola curve), where the performance is good during the start, worsens around the middle and then improves again towards the end.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-4-writing-efficient-code"><a class="header" href="#lecture-4-writing-efficient-code">Lecture 4: Writing Efficient Code</a></h1>
<p>Now that we know what a bottleneck is, how to identify one and model it, <strong>our challenge is build a system without a bottleneck</strong>, ie: one where all resources are equally utilised.
This is known as a <strong>balanced system</strong>.</p>
<p>Unfortunately, there is no such thing as a balanced system (only balanced sections of code). Different hardware optimisations have varying impact on code and therefore we say that balance is a function of code.</p>
<p>The <strong>fundamental tradeoff is between CPU and memory bandwidth efficiency</strong> and hence we distinguish between resource-bound code:</p>
<ul>
<li>Compute-bound</li>
<li>Memory-bound:
<ul>
<li>Latency-bound</li>
<li>Bandwidth-bound</li>
</ul>
</li>
</ul>
<p>We can influence these bottlenecks with different techniques, such as high-level techniques (choice of algorithm, memoisation, compression, etc.) and low-level techniques (our focus).</p>
<h2 id="compute-bound-code"><a class="header" href="#compute-bound-code">Compute-bound code</a></h2>
<p>Our primary objective is CPU efficiency. Generally, CPU-bound applications are those that:</p>
<ul>
<li>are poorly implemented</li>
<li>operate on small (cache-resident) datasets (ie: our bottleneck is not memory)</li>
<li>are math-heavy (esp. floating point math)</li>
<li>apply memory-oriented optimisations (ie: we have solved memory bottlenecks)</li>
</ul>
<p>Our <strong>target metric is wall clock time</strong>. Because wall clock time can be difficult to measure precisely, we often use certain proxy metrics such as stall cycles (caused by hazards) or CPI (cycles per instruction).</p>
<p><strong>Hazards</strong> cause stalls in the CPU pipeline and there are different types:</p>
<ul>
<li><strong>Control hazards</strong>: stalls due to data-dependent changes in the control flow (jumps and branches).</li>
<li><strong>Data hazards</strong>: stalls due to operands (data) not being available on time.</li>
<li><strong>Structural hazards</strong>: stalls due to a lack of physical execution resources (registers, execution ports/units, ...).</li>
</ul>
<p>We have already discussed hazards before (lecture 2) and they map nicely to the diagram below:</p>
<img src="assets/04_pipeline_hazards.png" width="500px">
<p>The ALU stall corresponds to a structural hazard, control hazards occur when no eligible instructions are ready (due to jumps) and abandonded instructions (due to misspeculation). Data stalls are caused by cache misses. Obviously, retired instructions are not hazardous.</p>
<p>Therefore, CPU optimisation is about mitigating these hazards and exploiting pipelined execution. Lets talk about some fundamental design decisions when it comes to pipelining.</p>
<h3 id="speculative-execution"><a class="header" href="#speculative-execution">Speculative execution</a></h3>
<p>Its purpose is to keep the pipeline full even if no instructions are eligible by executing instructions speculatively. This aims to address control hazards (branching). The other type of speculation is memory prefetching (covered in the memory-bound section below).</p>
<h3 id="superscalar-execution-dynamic-parallelism"><a class="header" href="#superscalar-execution-dynamic-parallelism">Superscalar execution (dynamic parallelism)</a></h3>
<p>The idea is to have multiple CPU pipelines within the same core. This means that different instructions can be in the same stage (eg: we can have two <code>MUL</code> instructions in the execution stage).</p>
<p>Modern CPUs tend to be four-way superscalar, ie: it has four different pipelines.</p>
<img src="assets/04_superscalar.png" width="400px">
<h3 id="out-of-order-execution"><a class="header" href="#out-of-order-execution">Out-of-order execution</a></h3>
<p>As a consequence of superscalar execution, out-of-order execution exploits the independence of instructions. </p>
<p>After decoding the instruction, the CPU will know if it has dependencies on other instructions or data. If instructions have <strong>no unsatisfied dependencies</strong>, it will move onto the execution stage directly. Therefore, we may execute instructions in a different order in which they arrive.</p>
<p>Out-of-order execution aims to address data and structural hazards.</p>
<h3 id="simd-static-parallelism"><a class="header" href="#simd-static-parallelism">SIMD (static parallelism)</a></h3>
<p>Another form of instruction-level parallelism is through SIMD instructions (Single Instruction Multiple Data). These instructions allow the CPU to perform the same operation on multiple data items at once, in a single cycle.</p>
<p>They use large specialised vector registers for different data types, allowing them to load multiple data items into a single register.</p>
<br>
<h3 id="improving-efficiency-with-partial-evaluation"><a class="header" href="#improving-efficiency-with-partial-evaluation">Improving efficiency with partial evaluation</a></h3>
<p>An important consideration to improving CPU efficiency is to write <strong>runtime predictable</strong> code for the critical path. We should evaluate code as early as possible and outside of the critical path whenever possible.</p>
<p>One technique is <strong>partial evaluation</strong> which consists of making the compiler work for you. We treat programs as multi-phased process. In every phase, you know more about the result so you can come up new programs that is more specialised tailored to the input. Examples are:</p>
<ul>
<li><strong>Function inlining</strong>: the compiler evaluates a function jump and places the code at the call site.</li>
<li><strong>JiT compilation</strong>: we start with higher level bytecode and compile it into native machine code.</li>
<li><strong>Symbolic programming</strong>: perform arithmetic on symbols rather than values (eg: <code>x/x</code> will always be 1).</li>
<li><strong>Constant expression evaluation</strong>: evaluate constant expressions at compile-time.</li>
</ul>
<p><strong>Example 1: constant evaluation</strong></p>
<pre><code class="language-c">int result(int input) {
  return input*3*5;
};

int result2(int input) {
  int three = 3-1;
  int five = 4+1;
  return input*three*five;
};

int three = 3;
int five = 5;
int result3(int input) {
  return input*three*five;
};
</code></pre>
<p>In <code>result2()</code>, the compiler figures out that variables <code>three</code> and <code>five</code> are always constant and replaces the operation with the values.</p>
<p>In <code>result3()</code>, the C compiler does not perform constant evaluation because it sees that <code>three</code> and <code>five</code> are variables available to other parts of the code and may be modified. It is important to understand the language semantics to know what constitutes a constant.</p>
<p><strong>Example 2: lifting expensive operations</strong></p>
<p>Any work that is executed often (such as in a loop) can be moved/lifted into a section where it executed seldomly. This can reduce control-flow hazards, since they now only occur once per loop rather than per iteration. A typical example is loop invariant motion, where we move loop invariant operations outside of the loop:</p>
<pre><code class="language-c">for (size_t i = 0; i &lt; N; i++) output[i] = 7*8;
</code></pre>
<p>becomes:</p>
<pre><code class="language-c">int tmp = 7*8; 
for (size_t i = 0; i &lt; N; i++) output[i] = tmp;
</code></pre>
<p>A related problem is <strong>loop specialisation</strong>. Suppose we want to scale a vector:</p>
<pre><code class="language-c">void scaleVector(int* input, size_t inputSize, int scale) {
  for(size_t i = 0; i &lt; inputSize; i++)
    input[i] *= scale;
}
</code></pre>
<p>The issue is that multiplications are quite expensive. Therefore, we can specialise the loop to avoid operations entirely (if <code>scale == 1</code>) or make them cheaper (use bit shifts).</p>
<pre><code class="language-c">void scaleVector2(int* input, size_t inputSize) {
  if(scale != 1) {    // avoid scale == 1 entirely
    if(scale == 2)
      (size_t i = 0; i &lt; inputSize; i++)
        input[i] &lt;&lt;= 1; // use cheap bitshift for scale == 2
    else
      (size_t i = 0; i &lt; inputSize; i++)
        input[i] *= scale;
  }
}
</code></pre>
<p>Unfortunately, this leads to code duplication. Instead, we can use metaprogramming to get the compiler to do the work for you. The idea here is to <strong>generate special cases at compile-time</strong> and apply optimisations for these cases. </p>
<p>C++ supports template metaprogramming and allows us to turn this...</p>
<pre><code class="language-c++">void scaleVector(int* input, size_t inputSize, int scale) {
  for(size_t i = 0; i &lt; inputSize; i++)
    input[i] *= scale;
};

int useIt(int* input, size_t size) {
  scaleVector(input, size, 2);
  scaleVector(input, size, 1);
  scaleVector(input, size, 0);
}
</code></pre>
<p>into this...</p>
<pre><code class="language-c++">template &lt;int scale&gt; void scaleVectorPE(int* input, size_t inputSize) {
  for(size_t i = 0; i &lt; inputSize; i++)
    input[i] *= scale;
};

int useIt(int* input, size_t size) {
  scaleVectorPE&lt;2&gt;(input, size);
  scaleVectorPE&lt;1&gt;(input, size);
  scaleVectorPE&lt;0&gt;(input, size);
}
</code></pre>
<p>Because the <code>scale</code> factor is known at compile-time, the compiler will be able to replace a multiplication by 2 with a bitshift, ignore the scale of 1 and set the input to 0 directly in the third case.</p>
<p>A useful pattern is to create a map that holds precomputed special cases for certain inputs. If the input is defined in the map, the precomputed result is obtained. Otherwise, we can resort to our runtime version of the function.</p>
<p><strong>Example 3: branch-free code</strong></p>
<p>We can go even further by writing branch-free code. We have seen how control-dependencies can cause hazards, such as below:</p>
<pre><code class="language-c">for(size_t i = 0; i &lt; inputSize; i++)
  if(input[i] &lt; high)
    output[outI++] = input[i];
</code></pre>
<p>We can be clever and manipulate how the <code>ouputI</code> index changes:</p>
<pre><code class="language-c">for(size_t i = 0; i &lt; inputSize; i++)
  output[outI] = input[i];
  outI += (size_t) (input[i] &lt; high);
</code></pre>
<p>We are now writing everything to the <code>output</code> array however we only move onto the next case if the condition holds. If it does not hold, <code>outI</code> does not change and the next value will overwrite the previous value (which is fine). In fact, this is beneficial for cache locality!
We can see the performance impact compared to the previous snippet (for a random array):</p>
<img src="assets/04_branch_free_graph.png" width="500px">
<p>We've converted this control dependency into a data dependency, so we need to be careful. If we find that branch missprediction is causing a problem, if-conversion is a good idea. Otherwise, it's probably not worth it (a lot of code is already very predictable, the case above is randomised). The bottom-line is: measure, then optimise!</p>
<p><strong>Example 4: SIMD vectorisation</strong></p>
<p>Compilers try to automatically vectorise your code. For a simple case like below...</p>
<pre><code class="language-c">for (size_t i = 0; i &lt; 1024; i++) out[i] = in1[i] * in2[i];
</code></pre>
<p>...the compiler will succeed. However, for more complicated cases we need to explicitly vectorise our code using intrinsics. Lets see an example below:</p>
<pre><code class="language-c++">#include &lt;immintrin.h&gt;

union v8f {    // either a float[8] or a SIMD word
  float floats[8];
  __m256 simdVec;
};

auto input1 = new int[bounds1]; // random data
auto input2 = new int[bounds2]; // random data

// Non-vectorised sum:
float sum = 0;
for (size_t i = 0; i &lt; bounds1; i++)
  sum += input2[input1[i]]


// Vectorised sum:
v8f sums{};
for (size_t i = 0; i &lt; bounds1 / 8; i++) {
  v8f values {
    // load values from memory using gather
    .simdVec = _mm256_i32gather_ps(input2, ((__m256i*)input1)[i], sizeof(int))
  };
  // perform addition directly on the simd register
  sums.simdVec = _mm256_add_ps(values.simdVec, sums.simdVec);
}
float sum = 0;
for (size_t i = 0; i &lt; 8; i++)
    sum += values.floats[i];
</code></pre>
<p>In the vectorised version, we obtain a x5 performance improvement than the scalar version. It is crucial that we keep our data in SIMD registers, otherwise we don't get the same benefits.</p>
<h2 id="memory-bound-code"><a class="header" href="#memory-bound-code">Memory-bound code</a></h2>
<p>Data hazards are caused by instructions that need to access data from memory but it results in a cache miss, producing a pipeline stall.
If the value has been accessed at some point previously, then this is called a <strong>capacity miss</strong>. Otherwise, this it is a <strong>compulsory miss</strong>.</p>
<p>Whenever we stall due to data hazards, we call the code <strong>memory-bound</strong>. We can be even more precise and describe two different situations whenever a stall occurs (due to data hazards):</p>
<ul>
<li><strong>Memory bandwidth bound</strong>: if the memory bus if fully utilised</li>
<li><strong>Memory latency bound</strong>: if the memory bus is not fully utilised</li>
</ul>
<p>We can apply different strategies based on the problem:</p>
<img src="assets/04_memory_bound_optimisation.png" width="300px">
<br>
<br>
<h3 id="compulsory-cache-misses"><a class="header" href="#compulsory-cache-misses">Compulsory cache misses</a></h3>
<p>Generally, the number of data-hazards (cache misses) when running a loop will be:</p>
<p>\(\frac{\text{data size}}{\text{cache line size}}\)</p>
<p>Luckily, CPUs support hardware prefetching whereby caches speculatively load the next cache line by recognising patterns and strides. This works well for regular memory accesses, but may break for irregular accesses like data-dependent accesses.</p>
<pre><code class="language-c">struct tuple { int x; int y; int z;};
int sumIt(tuple* input, long size, tuple* input2) {
  int sum = 0;
  for(size_t i = 0; i &lt; size; i++)
    sum += input2[input[i].x].y;

  return sum;
}
</code></pre>
<p>In the example above, the CPU knows the next cache line to be accessed for indexing into <code>input[i]</code> because it is a simple strided-accesses, however the index into <code>input2</code> is data-dependent. We can help the CPU through <strong>software prefetching</strong> via intrinsics:</p>
<pre><code class="language-c">struct tuple { int x; int y; int z;};
int sumIt(tuple* input, long size, tuple* input2) {
  int sum = 0;
  for(size_t i = 0; i &lt; size; i++){
    sum += input2[input[i].x].y;
    __builtin_prefetch(&amp;input2[input[i + 16].x]);
  }
  return sum;
}
</code></pre>
<p>We hint at the CPU that we are soon going to access the value within 16 iterations. Note that this happens <strong>asychronously</strong> at the hardware level.</p>
<h3 id="increasing-cache-line-utilisation"><a class="header" href="#increasing-cache-line-utilisation">Increasing cache-line utilisation</a></h3>
<p>Cache-line utilisation is defined as \(\frac{\text{data requested}}{\text{data loaded into cache}}\).
We can increase utilisation by changing the data layout in memory. We can change the <code>struct tuple</code> from the previous example to hold three arrays: one for <code>x</code>, <code>y</code> and <code>z</code>. This means that all <code>x</code>s are next to each other (and <code>y</code>s and <code>z</code>s):</p>
<pre><code class="language-c">struct tuple { int* x; int* y; int* z;};
int sumIt(tuple input, long size, tuple input2) {
  int sum = 0;
  for(size_t i = 0; i &lt; size; i++)
    sum += input2.y[input.x[i]];

  return sum;
}
</code></pre>
<p>This optimisation is called <em>Array-of-Structs</em> to <em>Struct-of-Arrays</em>.</p>
<h3 id="capacity-cache-misses"><a class="header" href="#capacity-cache-misses">Capacity cache misses</a></h3>
<p>Capacity-bound code suffers from <strong>thrashing</strong>: the larger the data we are accessing, the higher cost we are paying (because it doesn't fit in the cache).</p>
<p>This often happens when we have nested loops where we access two different regions of memory. A common solution to this issue is <strong>loop tiling</strong>, where we repeatedly access a cache line to keep it <em>hot</em>, so cache lines for one of the memory regions are never trashed.</p>
<p>(See lecture video at 1h 31m for good example)</p>
<br>
<p>In conclusion, if your code is:</p>
<ul>
<li><em>Bandwidth-bound</em>, then increase cache-line utilisation</li>
<li><em>Latency-bound</em>, then prefetch data</li>
<li><em>Capacity-bound</em>, then reduce the footprint/hot dataset</li>
</ul>
<h2 id="multicore-hazards-cache-coherency"><a class="header" href="#multicore-hazards-cache-coherency">Multicore hazards: cache coherency</a></h2>
<p>A final type of hazard to be aware about in a multicore environment is <strong>cache coherency</strong>. Whenever one core modifies a value in its cache, it needs to be updated in the other cores' caches. </p>
<p>Intel x86 CPUs have a QPI bus (Quick Path Interconnect) that connects caches directly to avoid having to go through the memory bus for cache coherency updates.</p>
<p>The cache coherency protocol used by most modern CPUs is <strong>MESI</strong>, which describes four different states in which a cache line can be in:</p>
<img src="assets/04_mesi.png" width="400px">
<ul>
<li><strong>Exclusive</strong>: a single core exclusively holds a cached copy of some data in its cache.</li>
<li><strong>Shared</strong>: when another core accesses the data, it creates a copy in its core and the cache line becomes shared.</li>
<li><strong>Modified</strong>: when one of the cores modifies the data, the state of the cache line in its core becomes modified.</li>
<li><strong>Invalid</strong>: the other cores' same cache lines become invalid.</li>
</ul>
<br>
<h2 id="tutorial-notes-1"><a class="header" href="#tutorial-notes-1">Tutorial notes</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-5-multicore-systems-and--parallelism"><a class="header" href="#lecture-5-multicore-systems-and--parallelism">Lecture 5: Multicore Systems and  Parallelism</a></h1>
<p>This lecture is about scaling up and adding parallelism to our program to improve performance.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>In 1974, an observation known as <strong>Dennard scaling</strong> says that <em>as transistors shrink, they become faster, consume less power and are cheaper</em>. However, due to physical and manufacturing limits, this observation ended in 2004.</p>
<p>On the other hand, <strong>Moore's Law</strong> is still true today (the number of transistors double every few years). To continue making good use of all these new transistors, we moved from single-core to multi-core in order to execute more instructions in parallel.</p>
<p>There are three forms of parallelism:</p>
<ul>
<li><strong>Data-level parallelism</strong>, such as vectorised instructions (SIMD).</li>
<li><strong>Instruction-level parallelism</strong>, via superscalar and out-of-order execution.</li>
<li><strong>Task-level parallelism</strong>, via threads, processes, GPU kernels...</li>
</ul>
<p>This lecture will focus on task-level parallelism: multithreading and multiprocessing.</p>
<p>Parallelism can be benefical for:</p>
<ol>
<li>Making an application <strong>faster</strong> (by breaking it down into parallel parts)</li>
<li>Making a system more <strong>cost-efficient</strong> (by sharing system resources or using multiple machines)</li>
</ol>
<h3 id="example-simple-cpu-provisioning"><a class="header" href="#example-simple-cpu-provisioning">Example: Simple CPU provisioning</a></h3>
<blockquote>
<p>Suppose a server with the following conditions:</p>
<ul>
<li>1 Gbps network card</li>
<li>1 request is 1 KB long</li>
<li>1 request takes 50 s to process</li>
</ul>
<p>Calculate the number of CPUs needed to serve this in a single machine.</p>
</blockquote>
<details>
  <summary>Click to see the answer</summary>
<p>The key to answering this question is to look at the dimensions/units of each quantity. The number of CPUs has no unit, so we need the units to cancel out:</p>
<p>Units: bits/sec  *  sec  *  1/bits<br>
Calculation: (1e9) * (50e-6) * (1/8000) = 6.25  -&gt;  7 CPUs needed</p>
</details>
<h2 id="multithreading"><a class="header" href="#multithreading">Multithreading</a></h2>
<p>Threads are concurrent streams of execution and share memory within a process. We will explore threads in more detail next.</p>
<h3 id="performance-analysis"><a class="header" href="#performance-analysis">Performance analysis</a></h3>
<p>When analysing the performance of a multithreaded application, it is important to understand the <strong>critical path</strong> (<em>the sequence of tasks determining the minimum time needed for an operation</em>) and any potential <strong>dependencies</strong> on non-parallel sections of the code.</p>
<p>It is important to optimise non-parellel sections of code instead of parallel code, as it will take up the most time in the critical path.</p>
<p>For example, if we have a program with the following runtime, clearly it is more important to optimise <code>c()</code> because it is the bottleneck of the system:</p>
<img src="assets/05_critical_path.png">
<h3 id="communication-and-synchronisation"><a class="header" href="#communication-and-synchronisation">Communication and synchronisation</a></h3>
<p>We will describe four types of communication and synchronisation:</p>
<ol>
<li>
<p><strong>Explicit sharing</strong>: all threads share process memory and the hardware handles memory updates through cache coherency protocols. We need to ensure we do not have data races through <em>atomic operations</em>, <em>mutexes/critical sections</em>, <em>condition variables</em> and other primitives. See <em>Theory and Practice of Concurrency</em> notes for more details on synchronisation primitives.</p>
</li>
<li>
<p><strong>False sharing</strong>: when data is accessed, the entire cache line is brought into cache, so if one thread writes to some memory, the cache line it is contained in will be invalidated in any other cores and will require it to be reloaded. We can avoid this using alignment/padding.</p>
</li>
</ol>
<h3 id="thread-management-models"><a class="header" href="#thread-management-models">Thread management models</a></h3>
<p>We will look at different patterns and models for managing threads:</p>
<ol>
<li>
<p><strong>On-demand</strong>: spawn a new thread on-demand for individual jobs. </p>
<ul>
<li> Very easy to implement </li>
<li> Very expensive when the num. threads &gt; num. CPUs due to contention and scheduling.</li>
</ul>
</li>
<li>
<p><strong>Fork-join</strong>: spawn a new thread on-demand to process for batched jobs. </p>
<ul>
<li> Effective when we are executing long computations in batches,</li>
<li> Limited to a fork-join scope scenario where batched execution is actually useful.</li>
</ul>
</li>
<li>
<p><strong>Work dispatching (thread pools)</strong>: a job generator cycles through a worker thread pool. </p>
<ul>
<li> Excellent for homogeneous jobs </li>
<li> Can lead to imbalance of work among the workers threads, since it is event-driven.</li>
</ul>
</li>
<li>
<p><strong>Work stealing (thread pools)</strong>: workers pull pending jobs from a shared job queue as soon as it becomes free. </p>
<ul>
<li> It avoids imbalance and therefore apt for heterogenous jobs</li>
<li> Enqueueing/dequeuing might be costly (due to contention and cache coherency traffic). We can use <em>adaptive batching</em> (dequeuing batches of jobs at once) to reduce the cost.</li>
</ul>
</li>
<li>
<p><strong>Streaming (producer/consumer)</strong>: similar to an &quot;assembly line&quot; across threads: we create one thread per function, the job is enqueued into the first function's input job queue and each function enqueues their result into the next function's input queue. This mimicks the idea of CPU pipelining.</p>
</li>
<li>
<p><strong>Staged event-driven (SEDA)</strong>: the modular logic of a system are divided into stages containing in/out queues and a thread pool. It supports massively concurrent systems and dynamic control, however it can lead to perfomance loss due to rescheduling and/or no cache locality. Also, due to the pipeline nature, the sequential execution will be limited by the slowest part of the pipeline.</p>
</li>
</ol>
<img src="assets/05_seda.png">
<h2 id="multiprocessing"><a class="header" href="#multiprocessing">Multiprocessing</a></h2>
<p>Multiprocessing is different to multithreading in that there is no shared memory and communication must be performed explicitly through OS-level interfaces.</p>
<p>It is useful for streaming scenarios with fully independent tasks and for scalable distributed pipelines because we can easily scale out processes into multiple machines (used by Spark).</p>
<h3 id="communication"><a class="header" href="#communication">Communication</a></h3>
<p>Without implicit shared memory, communication is key for:</p>
<ul>
<li>Performance: need to consider syscall, scheduling and memory copy overheads</li>
<li>Programmability: not as easy to call a function in another task (need to consider serialisation of data)</li>
</ul>
<p>Some solutions are:</p>
<ul>
<li>Explicit shared memory</li>
<li>Sockets</li>
<li>Pipes</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-6-system-and-device-performance"><a class="header" href="#lecture-6-system-and-device-performance">Lecture 6: System and Device Performance</a></h1>
<p>So far, we have mostly focused on CPUs and optimising for CPU performance. However, computer systems have a wide variety of components that can affect performance (memory, OS interfaces, device interfaces, etc.) and we wish to be able to be efficient on all fronts.</p>
<p>Computation and data are inextricable, so the movement of data through a system is crucial. Whether that is via IPC mechanisms, through IO devices or other low-level hardware mechanisms such as cache coherency or virtual address translation, it is important to understand that these all affect the overall performance of computation.</p>
<h2 id="hardware-related-challenges"><a class="header" href="#hardware-related-challenges">Hardware-related challenges</a></h2>
<p>To get the best performance out of systems, we need to understand the hardware. However, hardware is usually hidden away from software through various layers of abstraction. For example:</p>
<h3 id="instruction-and-memory-virtualisation"><a class="header" href="#instruction-and-memory-virtualisation">Instruction and memory virtualisation</a></h3>
<p>Deployed software often runs in virtualised environments, such as virtual machines or containers. This can lead to the virtual machine trapping into the host OS for different reasons, such as executing certain instructions. This can lead to order-of-magnitude overheads which can negatively affect performance.</p>
<p>A similar situation occurs with virtual memory. All virtual memory addresses need to be translated at runtime into physical addresses through TLBs, which are special hardware caches. This means that address translation is subject to TLB misses, making performance even harder to predict/model.
There are dedidcate <em>page walking</em> mechanisms that speculatively prefetch TLB entries by walking through the page table.</p>
<p>If we're inside a VM, then the costs are multiplied because we have &quot;nested&quot; page tables (the virtual OS + the host OS).</p>
<h3 id="memory-tiering"><a class="header" href="#memory-tiering">Memory tiering</a></h3>
<p>As we have seen many times before, memory is tiered in a hierarchy, with fast and small storage at the top and slow and large storage at the bottom of the pyramid.</p>
<p>Therefore, it is key to reduce misses and utilise caches closer to the CPU as much as possible.</p>
<h3 id="numa-and-cpu-access"><a class="header" href="#numa-and-cpu-access">NUMA and CPU access</a></h3>
<p>CPUs are connected to other CPUs through buses. For example, in Intel-based NUMA systems there is a dedicate QuickPath Interconnect (QPI) bus.</p>
<p>In the diagram below, each NUMA node consists of multiple cores and on-die caches, with a dedicated memory controller to access local memory and PCIe port to access I/O devices.</p>
<img src="assets/06_numa.png" width="500px">
<p>Note that cores on different CPUs can access each other's memory, but this is obviously much slower than access local memory.</p>
<p>In addition, different CPUs can access devices connected to different PCI ports, as well as devices accessing memory.</p>
<p>One feature offered by modern peripherals is PCIe multi-homing. As the name indicates, this means that the same device can be connected to various PCIe ports and maps memory addresses to the PCIe connection. </p>
<h3 id="cpu-and-device-interaction"><a class="header" href="#cpu-and-device-interaction">CPU and device interaction</a></h3>
<p>The traditional modes of CPU-device interaction is via interrupts and memory-mapped I/O (MMIO).</p>
<p>MMIO maps a region of addresses to the PCIe bus on which the device is connected. Every CPU write is sent as data to the device, whereas CPU reads act as a way of receiving data from the device. It is much faster to poll MMIO addresses instead of waiting for interrupts. However, it can lead to high PCIe traffic and latency (around 1s, which is much slower compared to RAM and cache).</p>
<h2 id="io-interfaces"><a class="header" href="#io-interfaces">I/O interfaces</a></h2>
<p>Let us look at the different I/O interfaces offered by most operating systems (in particular, Linux).</p>
<p><strong>Blocking I/O</strong> is the default behaviour for the traditional system calls (<code>open</code>, <code>close</code>, <code>read</code>, <code>write</code>, ...) as it is simple to use and understand. However, it is a blocking operation and is expensive:</p>
<ol>
<li>Trap into kernel model on syscall</li>
<li>Copy user data into kernel</li>
<li>Block and reschedule thread if I/O operation not available</li>
<li>Copy kernel data into user space</li>
</ol>
<p>A more efficient approach can be taken via <strong>non-blocking I/O APIs</strong> and generally has two flavours:</p>
<ul>
<li>
<p><em>Asynchronous</em> (designed for storage I/O): </p>
<ol>
<li>Setup the I/O operation(s) with a syscall</li>
<li>Do something else</li>
<li>Kernel signals the end of the syscall</li>
<li>React to the end of the syscall</li>
</ol>
</li>
<li>
<p><em>Events</em> (designed for network I/O):</p>
<ol>
<li>Tell kernel what operation we want to perform on a file descriptor</li>
<li>Do something else</li>
<li>Kernel signals when the operation is available</li>
<li>Operate on the file descriptor</li>
</ol>
</li>
</ul>
<h3 id="linux-asynchronous-io"><a class="header" href="#linux-asynchronous-io">Linux: asynchronous I/O</a></h3>
<p>Useful when we want to execute something else while disk file is being read or written.</p>
<ul>
<li>
<p>Submit a list of operations: </p>
<p><code>io_submit(aio_context_t ctx_id, long nr, struct iocb** iocbpp)</code></p>
</li>
<li>
<p>Wait for events to complete: </p>
<p><code>io_getevents(aio_context_t ctx_id, long min_nr, long nr, struct io_event *events)</code></p>
</li>
</ul>
<p>There are other operations to setup and cancel a list of operations.</p>
<h3 id="linux-event-based-io-with-epoll"><a class="header" href="#linux-event-based-io-with-epoll">Linux: event-based I/O (with <code>epoll</code>)</a></h3>
<p>Useful when we want to execute something else while there is no data to read from a network socket. </p>
<p>Linux offers the <code>epoll</code> API which is efficient and easy to understand, as it has evolved from <code>select</code> and <code>poll</code>. You first register the file descriptor for the reading/writing poll set and then wait for any of the fds in such sets to be ready:</p>
<ul>
<li>
<p>Register a file descriptor into the read/write poll set: </p>
<p><code>epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)</code></p>
</li>
<li>
<p>Wait for any fd to be ready: </p>
<p><code>epoll_wait(int epfd, struct epoll_event *events, int maxevents)</code></p>
</li>
</ul>
<p>There are other operations to setup and cancel a list of operations.</p>
<p><strong>Note:</strong> you must first set the FDs to use non-blocking I/O using <code>fcntl</code> and the <code>O_NONBLOCK</code> flag.</p>
<h3 id="linux-io_uring"><a class="header" href="#linux-io_uring">Linux: <code>io_uring</code></a></h3>
<p>The <code>io_uring</code> API covers both storage and network non-blocking operations. It works by setting up a request and response queue which is shared between the process and kernel. Queues can be manipulated and polled in user-level code, meaning that no syscalls are required to check if a response is ready. In addition, it facilitates zero-copy because the queue's memory is shared.</p>
<h3 id="direct-device-access"><a class="header" href="#direct-device-access">Direct device access</a></h3>
<p>As we have seen, performing I/O operations through the OS can be expensive. The next extreme would be to directly access hardware from user space and completely bypass the OS kernel. User code can use specialised libraries and protocols to access devices which are memory-mapped into the process' memory.</p>
<p>Examples of this are DPDK for network I/O and SPKD for storage I/O.</p>
<h2 id="design-patterns-for-io-programming"><a class="header" href="#design-patterns-for-io-programming">Design patterns for I/O programming</a></h2>
<p>This section covers some of the now obselete and more modern approaches to I/O programming for servers.</p>
<h3 id="one-thread-per-task-obsolete"><a class="header" href="#one-thread-per-task-obsolete">One thread per task (obsolete)</a></h3>
<ol>
<li>Main thread listens for new connections</li>
<li>A new connection is accepted</li>
<li>Spawns a new thread to handle the new connection</li>
<li>The new thread does all blocking operations (read from socket, execute logic, read/write disk, write to socket).</li>
</ol>
<h3 id="single-thread-pool-obsolete"><a class="header" href="#single-thread-pool-obsolete">Single thread pool (obsolete)</a></h3>
<p>The only difference with the previous approach is that we use a thread pool and recycle threads for each new connection, rather than spawning a new one. This eliminates the cost of thread creation and does not process new connections if the thread pool is exhausted.</p>
<h3 id="separate-thread-pools"><a class="header" href="#separate-thread-pools">Separate thread pools</a></h3>
<p>Here, we maintain a separate thread pool for long operations (blocking I/O). We keep the main thread busy with short operations and hand-off I/O to worker threads.</p>
<ol>
<li>Receive and process the request in the main thread</li>
<li>On a potentially long (blocking) operation, enqueue the socket fd to the worker queue</li>
<li>Workers poll the input queue</li>
<li>One worker takes the fd and executes the descriptor (eg: disk read)</li>
<li>Worker pushes the response back to main thread's queue</li>
<li>The main thread continues working using the response</li>
</ol>
<p>This is typically achieved using callbacks: the main thread function continues after the I/O worker is done.</p>
<h3 id="event-based-systems"><a class="header" href="#event-based-systems">Event-based systems</a></h3>
<p>These are designed for high throughput and low latency servers. We have many threads that operate on the same <code>epoll</code>/<code>AIO</code> context. This is typically implemented as a state machine that transitions across incoming events.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
