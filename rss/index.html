<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Alex Constantin-Gómez]]></title><description><![CDATA[Some posts, tutorials and notes for self reference on different topics of software engineering that interest me.]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Alex Constantin-Gómez</title><link>http://localhost:2368/</link></image><generator>Ghost 4.32</generator><lastBuildDate>Sun, 10 Apr 2022 11:26:30 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Performance comparison between virtual calls and static dispatch]]></title><description><![CDATA[<p></p><h3 id="background">Background</h3><p>Virtual functions in C++ is the most popular way to achieve polymorphism. Any class with a virtual member function declaration will cause the compiler to add an a pointer to every instance of the class. This pointer is known as the <em>virtual table pointer</em>, which points to a table</p>]]></description><link>http://localhost:2368/performance-comparison-of-virtual-calls-and-static-dispatch/</link><guid isPermaLink="false">6252a96bbd2eee15d02f77fb</guid><dc:creator><![CDATA[Alex Constantin-Gomez]]></dc:creator><pubDate>Sun, 10 Apr 2022 11:22:29 GMT</pubDate><content:encoded><![CDATA[<p></p><h3 id="background">Background</h3><p>Virtual functions in C++ is the most popular way to achieve polymorphism. Any class with a virtual member function declaration will cause the compiler to add an a pointer to every instance of the class. This pointer is known as the <em>virtual table pointer</em>, which points to a table of virtual functions for that particular class. Thus, whenever a virtual function is invoked, the pointer is dereferenced to jump to the correct function. This all occurs at runtime, so it is known as dynamic dispatch (or dynamic polymorphism). Most OOP languages such as Java follow the same strategy; in fact, it is the only form of dispatching that is supported (for instance methods). However, C++ allows for another form of dispatching: <strong>static dispatch</strong>.</p><p>Static dispatch essentially means that the compiler knows in advance what method to jump to. Not only does this avoid the extra layer of indirection (because there is no vtable pointer), it also allows for methods to be inlined which gives room of even more performance benefits (as opposed to virtual functions which cannot be inlined). An important but final benefit is that less space is used per instance because we do not need to store a pointer. This can be beneficial for small objects, since more instances can fit in cache.</p><p> In C++, static polymorphism is achieved through the <strong>Curiously Recurring Template Pattern (CRTP)</strong>.</p><p>As a refresher, the template pattern allows you to delegate specific behaviour in an algorithm to subclasses. With CRTP, the base class is a template class where the template type is a derived class, so it has direct access to the hook method implemented in the derived class. The derived class then inherits from the base class specialised with the derived class itself.</p><h3 id="comparison">Comparison</h3><!--kg-card-begin: markdown--><p>Let&apos;s start with dynamic dispatch, by creating a simple <em>interface</em> that we want our subclasses to implement:</p>
<pre><code class="language-c++">struct IShape {
    virtual double area() = 0;
    virtual void scale(int factor) = 0;
};

class Square : public IShape {
    double m_length;
    
public:
    Square(double len) : m_length(len) {}
    double area() override { return m_length * m_length; }
    void scale(int factor) override { m_length *= factor; }
};
</code></pre>
<p>Now, let&apos;s implement the equivalent code using CRTP:</p>
<pre><code class="language-c++">template&lt;typename Impl&gt;
class IShapeCRTP {
public:
    double area() { return impl()-&gt;area(); }
    
    void scale(int factor) { impl()-&gt;scale(factor); }
private:
    Impl* impl() { return static_cast&lt;Impl*&gt;(this); }
};

class SquareCRTP : public IShapeCRTP&lt;SquareCRTP&gt; {
    double m_length;
    
public:
    SquareCRTP(double len) : m_length(len) {}
    double area() { return m_length * m_length; }
    void scale(int factor) { m_length *= factor; }
};
</code></pre>
<p>Now, let&apos;s write the benchmarks for both versions. I am using Google Benchmark:</p>
<pre><code class="language-c++">#include &lt;benchmark/benchmark.h&gt;

static void DynamicDispatchBenchmark(benchmark::State &amp;state) {
    IShape* square = new Square(10);
    for (auto _ : state) {
        square-&gt;scale(3);
    }
    delete square;
}

static void StaticDispatchBenchmark(benchmark::State &amp;state) {
    IShapeCRTP&lt;SquareCRTP&gt;* square = new SquareCRTP(10);
    for (auto _ : state) {
        square-&gt;scale(3);
    }
    delete square;
}

BENCHMARK(DynamicDispatchBenchmark);
BENCHMARK(StaticDispatchBenchmark);

BENCHMARK_MAIN();
</code></pre>
<p>The results on my laptop are:</p>
<pre><code>DynamicDispatchBenchmark       2.64 ns
StaticDispatchBenchmark        1.17 ns
</code></pre>
<p>That&apos;s over x2 speedup! Obviously, as with any microbenchmark, you need to take the results with a pinch of salt, since there are many factors that could affect performance in a real application which are not taken into account in an isolated example like the one above. You can see the assembly output <a href="https://godbolt.org/z/f5e4a9Kfn">here</a> for yourself.</p>
<p>Either way, from both a practical and theoretical point of view, we see that static dispatch via CRTP provides several benefits that can lead to performance increases, compared with dynamic dispatch.</p>
<!--kg-card-end: markdown--><p></p>]]></content:encoded></item><item><title><![CDATA[Intro to lock-free programming]]></title><description><![CDATA[In this post, I attempt to explain the fundamental concepts required to write lock-free programs.]]></description><link>http://localhost:2368/lock-free-programming/</link><guid isPermaLink="false">61d0cd30135cc21a67fab03d</guid><dc:creator><![CDATA[Alex Constantin-Gomez]]></dc:creator><pubDate>Mon, 03 Jan 2022 23:30:05 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p><small><strong>Disclaimer</strong>: Before starting, I want to highlight that I am fairly new to lock-free programming and still have a lot to learn. This post is intended for me to self-study and revisit in the future if needed, and is based on what I have learnt so far as a CS undergraduate and researching online.</small></p>
<!--kg-card-end: markdown--><h2 id="introduction">Introduction</h2><p>Lock-free programming is often described as programming with locking and unlocking mutexes. While this is true, it is only a &quot;subset&quot; of the actual definition. A lock-free program/algorithm refers to code that cannot cause the system to lock up, such as a deadlock or livelock. In other words, there is no possible scheduling of threads and executions that would lead to a <em>locked-up</em> state. This means that the whole system makes progress as a whole, ie: at least one thread makes progress. As long as a program is able to invoke lock-free operations, the number of completed invocations keeps increasing no matter what.</p><p>Another common term in the world of lock-free programming is <em>wait-free</em>. The latter refers to a program where every thread makes progress and execute in a finite number of steps, regardless of contention. In that sense, <em>wait-free</em> is a strict subset of <em>lock-free</em> and generally, writing wait-free code is harder. However, this post will focus on lock-free programming.</p><p>As part of lock-free programming, we need to go through atomics, memory models and orderings.</p><h2 id="memory-models">Memory models</h2><p>A memory model describes how memory is read from and written to and in what order. This typically means <em>how reads and writes may be reordered</em> and <em>what memory barriers</em> <em>can be used</em> to prevent certain reorderings. We can choose to describe how memory behaves at a hardware level or at a software level.</p><h3 id="hardware-memory-models">Hardware memory models</h3><p>The hardware memory model will characterise the CPU&apos;s ability to reorder memory operations. Stronger memory models refer to those that provide more guarantees about how memory operations may be reordered (ie: the stronger the model, the less types of reorderings it admits). The strongest memory model is sequential consistency (SC) however no architecture implements SC at a hardware level because it is too expensive and prevents many optimisations.</p><p>In <strong>x86</strong>, the memory model is known as <strong>Total Store Order</strong> (TSO). It allows write-read reordering on different memory locations, ie: <em>a read from location x can be reordered before a write to y</em>. This effect is achieved through <strong>store buffering</strong>: all writes are enqueued in a per-core store buffer, allowing later reads to &quot;overtake&quot; these buffered writes. These buffered writes will eventually be dequeued at non-deterministic times (a type of out-of-order execution), unless a memory barrier is issued to indicate that all writes should be immediately flushed.<br><br>A weaker memory model is <strong>Partial Store Order</strong> (PSO), which is like TSO but allows write-write reordering, and an even weaker memory model is <strong>Relaxed Store Ordering</strong> (RSO), which allows even more reorderings. These weaker models can be found on SPARC, ARM and Alpha architectures. Therefore, x86 (TSO) is actually known to be a fairly strong memory model, as it only allows one type of reordering. </p><p>Because RSO is weaker than PSO and PSO is weaker than TSO, any TSO interleaving is also a possible PSO and RSO interleaving, but not necessarily the other way around.</p><h3 id="software-memory-models">Software memory models</h3><!--kg-card-begin: markdown--><p>We can abstract away from the hardware and instead look at how we want our software to behave with regard to memory reorderings. Each architecture will have a set of memory barriers in their instruction set (<code>mfence</code> on x86) which allow us to prevent reordering in our code (more on memory barriers in the next section).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>We may choose to manually issue memory fence instructions, however it is common for the compiler to do that work for us when using a higher-level constructs, such as the <code>std::atomic</code> library in C++. We can tag atomic operations with a specific memory order and the compiler will emit the necessary memory barriers.</p>
<!--kg-card-end: markdown--><p>An idealised and high-level memory model is <strong>sequential consistency</strong>. SC corresponds to interleaving semantics and makes it easy to reason about your program&apos;s correctness. However, as mentioned earlier, CPUs do not implement SC at the hardware level because it prevents optimisations. The way SC memory operations are achieved is by emitting a full memory barrier immediately after it, so that other threads see the effect of the operation. In C++, all atomic operations will work under SC semantics.</p><p>On the other extreme, we can choose not to insert any memory barriers at all and let the CPU reorder the instructions as it sees fit. In C/C++, this is known as <strong>relaxed ordering</strong> (not related to the RMO hardware model). The semantics of relaxed-tagged atomic operations will correspond to the memory model of the hardware.</p><p>Somewhere in between the two previous models, we have <strong>acquire-release</strong> ordering. This model will provide an ordering between atomic stores and loads. An atomic store can be tagged as release and an atomic load can be tagged as acquire. <br><strong>Acquire</strong> semantics prevent memory reordering of the read-acquire operation with any other read or write operation that follows it.<br><strong>Release</strong> semantics prevent memory reordering of the write-release operation with any other read or write operation that precedes it.<br>Depending on the architecture, acquire-release operations may require fewer memory barriers than SC.</p><p>Acquire-release orderings are perfect for message passing and can be used to implement <a href="https://github.com/thealexcons/spinlock/blob/master/active_backoff_spinlock.h">spinlocks</a>. In fact, the term &quot;acquiring/releasing a lock&quot; actually comes from the acquire-release memory semantics.</p><h3 id="types-of-memory-barriers">Types of memory barriers</h3><p>In lock-free programming, we want to make sure that the correct memory barriers are used/emitted by the compiler so that our program behaves correctly and enforces an ordering of instructions.</p><p>Since we have four types of reordering (read-read, read-write, write-read and write-write), we would require four memory fences. In practice, we usually don&apos;t see all four. For example, Linux provides three barriers that are sufficient for its supported architectures:</p><!--kg-card-begin: markdown--><ul>
<li><code>smp_mb()</code>, <strong>full memory barrier</strong> that orders both reads and writes (any operation before the barrier will be committed to memory before any operation after the barrier).</li>
<li><code>smp_rmb()</code>, <strong>read memory barrier</strong> that orders reads.</li>
<li><code>smp_wmb()</code>, <strong>write memory barrier</strong> that orders writes.</li>
</ul>
<!--kg-card-end: markdown--><p>For more details on memory barriers and hardware, I would strongly recommend reading <a href="http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf">this paper</a> by Paul E. McKenney.</p><h2 id="atomic-operations">Atomic operations</h2><p>In a multithreaded program where multiple threads read/write shared memory, we need to somehow synchronise accesses to avoid unwanted data races. This can be done with mutexes or with the appropriate synchronisation provided by atomic operations.</p><!--kg-card-begin: markdown--><p>An atomic operation is one that manipulates memory in an indivisible way, such that no threads can observe the operation half-complete. On most processors nowadays, many operations are already atomic. For example, reads and writes of native types (those that fit in the memory bus, such as integers) which are aligned are atomic. For example, on x86 reads and writes to 64-bit-sized locations are atomic. This why you can write code such as <code>std::atomic&lt;int&gt; x</code> or <code>std::atomic&lt;long&gt; y</code> in C++: the compiler will align these memory locations and just use standard <code>MOV</code> operations to atomically read the values. You can use <code>std::atomic&lt;T&gt;::is_lock_free</code> in C++ to verify that the type T can be atomically read/written without using locks.</p>
<!--kg-card-end: markdown--><p>You can go a step further than loading and storing data atomically by using <strong>Read-Modify-Write</strong> (RMW) operations. As the name indicates, they allow you to perform more complex transactions atomically. They&apos;re crucial in lock-free code when multiple threads want to write to the same location because when they attempt an RMW on that location, the RMWs effectively become linearisable and execute one-at-a-time, in a line.</p><h2 id="compare-and-swap">Compare-And-Swap</h2><p>The most important RMW operation is compare-and-swap (CAS). A CAS operation will compare the shared atomic value with an expected value, and if equal, it will update the shared location with a new value. In this case, the CAS is successful and returns true. If not equal, we say the CAS is unsuccessful and it returns false.</p><!--kg-card-begin: markdown--><p>It is the only RMW that is absolutely essential, since every other RMW such as <code>fetch_add()</code> or <code>exchange()</code> can be implemented using CAS.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Often, we use <strong>CAS loops</strong> to repeatedly attempt a transaction until successful. This pattern typically involves:</p>
<ol>
<li>Copying a shared variable to a local variable (private to other threads)</li>
<li>Performing some speculative work using the local variable</li>
<li>Attempting to publish any changes to the shared variable using CAS</li>
</ol>
<p>For example, we can implement a seemingly complex operation atomic and lock-free using a CAS loop. Below, we implement a function that atomically doubles a value if odd or increments it if even, and returns the old value:</p>
<pre><code class="language-c++">int atomicDoubleOrIncrement(std::atomic&lt;int&gt;&amp; shared) {
    int old = shared.load();
    int new;
    do {
        if (old % 2 != 0) {
            new = old * 2;
        } else {
            new = old + 1;
        }
    } while (!shared.compare_exchange_strong(old, new));
    return old;
}
</code></pre>
<p>The idea is that if the CAS fails due to a modification to <code>shared</code> by another thread, the local variable <code>old</code> gets updated to be the latest value of <code>shared</code> (in C++, if a CAS fails then it will update the expected parameter to contain the latest value) and we try again. You can think of the body of a CAS loop as a protected section that cannot be seen by other threads because we perform work using the local variable.</p>
<p>The code above qualifies as lock-free because:</p>
<ol>
<li>if the CAS succeeds, then we break from the loop and the thread makes progress</li>
<li>if the CAS fails, then it must be because another thread updated the value first and their CAS was successful, so there is another thread that makes progress</li>
</ol>
<p>However, this function is not wait-free because only one thread can make progress at any given time and a thread&apos;s progress will depend on contention.</p>
<!--kg-card-end: markdown--><h2 id="closing-thoughts">Closing thoughts</h2><p>Lock-free programming is hard, as it requires you to have a good understanding of a variety of topics that go all the way down to the hardware. I am still a novice in this area however I am really enjoying learning about it and writing this post has helped solidify my understanding of memory models, atomics, instruction reordering, etc.<br></p>]]></content:encoded></item><item><title><![CDATA[Writing custom memory allocators in C++]]></title><description><![CDATA[We look at how to implement custom memory allocators in C++ and their benefits.]]></description><link>http://localhost:2368/custom-memory-allocators/</link><guid isPermaLink="false">61cc8a2de7f77539899bef87</guid><dc:creator><![CDATA[Alex Constantin-Gomez]]></dc:creator><pubDate>Thu, 30 Dec 2021 12:54:15 GMT</pubDate><content:encoded><![CDATA[<p></p><h2 id="introduction">Introduction</h2><!--kg-card-begin: markdown--><p>An essential part of memory management is how memory is allocated and eventually deallocated. By default, memory in C++ programs is allocated using the <code>new</code> keyword and deallocated using the <code>delete</code> keyword. However, sometimes we want more control over how and where objects are allocated/deallocated to avoid issues like fragmentation.</p>
<p>The C++ standard library allows programmers to write custom allocators which can be used by STL containers for dynamic memory allocation, rather than using the standard allocator.</p>
<p>Allocators can be used to improve performance-related issues such as fragmentation, per-thread allocation and NUMA-friendly allocation.</p>
<p>We will see some examples in this post and their benefits, but before we should mention the different properties and requirements an allocator should have in C++.</p>
<!--kg-card-end: markdown--><h2 id="the-allocator-api">The allocator API</h2><!--kg-card-begin: markdown--><p>In C++, an allocator is a template class that allocates and deallocates memory for a specific type T. There are two types of allocators:</p>
<ul>
<li><strong>Equal</strong> allocators: two equal allocators can be used to allocate and deallocate memory for a type T interchangeably. These are usually <strong>stateless</strong> allocators.</li>
<li><strong>Unequal</strong> allocators: two unequeal allocators cannot be used to allocate and deallocate memory interchangeably. These are usually <strong>stateful</strong> allocators.</li>
</ul>
<p>An allocator class should offer a <code>T* allocate(size_t n)</code> method to allocate <code>n</code> number of objects of type <code>T</code> and a <code>void deallocate(T* p, size_t n)</code> method to deallocate an object of type <code>T</code>.</p>
<p>Additionally, we need to provide an empty copy constructor using a template of type <code>U</code> for full compatibility with STL containers, because the container may also need to allocate internal objects (such as linked list nodes) in addition to objects of type <code>T</code>.</p>
<p>The most simple allocator using <code>malloc()</code> can be implemented as follows:</p>
<pre><code class="language-c++">template &lt;typename T&gt;
class SimpleAllocator {
public:
    using value_type = T;

    SimpleAllocator() = default;
    
    template &lt;typename U&gt;
    SimpleAllocator(const SimpleAllocator&lt;U&gt;&amp; other) {
        (void) other;
    }
    
    T* allocate(size_t n) {
        auto ptr = static_cast&lt;T*&gt;(malloc(sizeof(T) * n));
        if (ptr)
            return ptr;
            
        throw std::bad_alloc();
    }
    
    void deallocate(T* ptr, size_t n) {
        (void) n;
        free(ptr);
    }
};
</code></pre>
<p>Because this is a stateless allocator and just uses <code>malloc()</code>, this is an equal allocator, so it is good practice to define the following equality operators for our allocator:</p>
<pre><code class="language-c++">template &lt;typename T, typename U&gt;
bool operator==(const SimpleAllocator&lt;T&gt;&amp; a1, const SimpleAllocator&lt;U&gt;&amp; a2) {
    (void) a1; (void) a2;
    return true;
}

template &lt;typename T, typename U&gt;
bool operator!=(const SimpleAllocator&lt;T&gt;&amp; a1, const SimpleAllocator&lt;U&gt;&amp; a2) {
    (void) a1; (void) a2;
    return false;
}
</code></pre>
<p>In C++17, you can avoid manually writing the two equality operators above by adding the property <code>using is_always_equal = std::true_type</code> if the allocator is equal (or <code>std::false_type</code> if unequal).</p>
<p>Because <code>SimpleAllocator</code> is an equal allocator, it is legal to do the following:</p>
<pre><code class="language-c++">SimpleAllocator&lt;double&gt; a1;
SimpleAllocator&lt;double&gt; a2;

double* ptr = a1.allocate(1);   // allocate a double with a1
a2.deallocate(ptr, 1);          // deallocate the memory with a2
</code></pre>
<!--kg-card-end: markdown--><h2 id="example-1-a-stateless-cache-aligned-allocator">Example 1: A stateless cache-aligned allocator</h2><p>As an extension to our first stateless allocator which does nothing interesting, we will now implement an allocator that actually does something useful. In this case, our allocator will automatically eliminate false sharing in an STL container being accessed by multiple threads.</p><p>Briefly, the solution to false sharing is to align the shared memory locations such that they end up in different cache lines. On x86 CPUs, L1 cache lines are 64 bytes, so our allocator should allocate objects at 64 byte boundaries. Here is the code:</p><!--kg-card-begin: markdown--><pre><code class="language-c++">template &lt;typename T, size_t Alignment = 64&gt;
class CacheAlignedAllocator {
public:
    using value_type = T;
    using is_always_equal = std::true_type;

    template &lt;typename U&gt;
    struct rebind {
        using other = CacheAlignedAllocator&lt;U, Alignment&gt;;
    };

    CacheAlignedAllocator() = default;

    template &lt;typename U&gt;
    CacheAlignedAllocator(const CacheAlignedAllocator&lt;U, Alignment&gt;&amp; other) {
        (void) other;
    }

    T* allocate(size_t n) {
        auto ptr = static_cast&lt;T*&gt;(aligned_alloc(Alignment, sizeof(T) * n));
        if (ptr)
            return ptr;

        throw std::bad_alloc();
    }

    void deallocate(T* ptr, size_t n) {
        (void) n;
        free(ptr);
    }
};
</code></pre>
<p>For allocation, we use C++&apos;s builtin function <code>aligned_alloc()</code>. Everything else is the same, except for the <code>struct rebind</code>. Typically, this struct is generated by the compiler automatically for us but because our allocator takes in more than one template argument (in case the user wants to supply a different alignment amount), we must manually define our own rebind struct (which is used by STL containers to create new allocators without having to call the copy constructor).</p>
<p>You can now specify the custom allocator as a policy template argument to an STL container and verify that all allocations are indeed 64-byte aligned (by inserting a print statement in the <code>allocate</code> method):</p>
<pre><code>std::vector&lt;int, CacheAlignedAllocator&lt;int&gt;&gt; vec;
for (int i = 0; i &lt; 5; i++) {
    vec.emplace_back(i);
}
</code></pre>
<p>Running <a href="https://github.com/thealexcons/cpp-allocators/blob/master/cache_aligned_allocator.cc">this benchmark</a> I have prepared to demonstrate the effects of false sharing in a multi-threaded program, for 10 iterations, I obtain:</p>
<pre><code>        std::allocator mean: 2470 ms
CachedAlignedAllocator mean: 2192 ms
</code></pre>
<p>Measuring latency in this case may not be fully representative because it takes into account thread creation and context switching which adds jitter to the result. However, you can look at the L1 cache misses using <code>perf</code> on Linux and verify that we get less L1 cache misses using the custom allocator.</p>
<!--kg-card-end: markdown--><h2 id="example-2-a-stateful-pool-allocator">Example 2: A stateful pool allocator</h2><p>Now, we look at a classic use case of custom memory allocators: a pool allocator. The goal of a pool-based allocator is to quickly allocate memory for a fixed-type objects while reducing internal fragmentation of memory.</p><!--kg-card-begin: markdown--><p>Pool allocators work by allocating large blocks of memory in advance and dividing this block for individual allocations. This means that memory allocation is much faster than calling <code>malloc()</code>, which is slow.</p>
<p>Because a pool allocator has to manage a list of blocks, it is a <strong>stateful</strong> allocator (and therefore unequal).<br>
Firstly, we need to create a <code>Pool</code> class that manages the memory of chunks of a given size. We use a stack of addresses to quickly pop an available address when we need to allocate an object and push back a newly available address when we deallocate an object at that address.</p>
<pre><code class="language-c++">template &lt;size_t BlockSize, size_t ReservedBlocks = 0&gt;
class Pool {
private:
    size_t size_;
    std::stack&lt;void *&gt; addrs_;
    std::stack&lt;std::unique_ptr&lt;uint8_t[]&gt;&gt; blocks_;

public:
    explicit Pool(size_t size) : size_(size) {
        for (size_t i = 0; i &lt; ReservedBlocks; i++) {
            add_more_addresses();
        }
    }

    void* allocate() {
        if (addrs_.empty()) {
            add_more_addresses();
        }

        auto ptr = addrs_.top();
        addrs_.pop();
        return ptr;
    }

    void deallocate(void *ptr) {
        addrs_.push(ptr);
    }

    void rebind(size_t size) {
        if (!(addrs_.empty() &amp;&amp; blocks_.empty())) {
            std::cerr &lt;&lt; &quot;Cannot call Pool::rebind() after alloc\n&quot;;
            abort();
        }

        size_ = size;
    }

private:
    // Refill the address stack by allocating another block of memory
    void add_more_addresses() {
        auto block = std::make_unique&lt;uint8_t[]&gt;(BlockSize);
        auto total_size = BlockSize % size_ == 0 ? 
                            BlockSize : BlockSize - size_;

        // Divide the block into chunks of size_ bytes, and add their addrs
        for (size_t i = 0; i &lt; total_size; i += size_) {
            addrs_.push(&amp;block.get()[i]);
        }

        // Keep the memory of the block alive by adding it to our stack
        blocks_.push(std::move(block));
    }
};
</code></pre>
<p>In our constructor, we specify the fixed-size number of bytes we want to allocate (this will be passed in later as <code>sizeof(T)</code>) and we reserve memory blocks if specified by the template parameter.</p>
<p>Importantly, we use a <code>std::unique_ptr</code> to automatically free the memory when we are done using the allocator.</p>
<p>The <code>allocate()</code> method will simply return the first address on the stack in O(1) time (unless the stack is empty) and our <code>deallocate()</code> just puts back the address onto the stack.<br>
We also provide a <code>rebind()</code> method to keep STL containers happy.</p>
<p>Now, we need to create the actual <code>PoolAllocator</code> class which manages a <code>Pool</code> instance:</p>
<pre><code class="language-c++">template &lt;typename T, size_t BlockSize = 4096, size_t ReservedBlocks = 0&gt;
class PoolAllocator {
private:
    using PoolType = Pool&lt;BlockSize, ReservedBlocks&gt;;
    std::shared_ptr&lt;PoolType&gt; pool_;

public:
    using value_type = T;
    using is_always_equal = std::false_type;

    template &lt;typename U&gt;
    struct rebind {
        using other = PoolAllocator&lt;U, BlockSize, ReservedBlocks&gt;;
    };

    PoolAllocator() : pool_(std::make_shared&lt;PoolType&gt;(sizeof(T))) {}

    // Rebind copy constructor
    template &lt;typename U&gt;
    PoolAllocator(const PoolAllocator&lt;U&gt;&amp; other) : pool_{other.pool_} {
        pool_-&gt;rebind(sizeof(T));
    }

    PoolAllocator(const PoolAllocator&amp; other) = default;
    PoolAllocator(PoolAllocator&amp;&amp; other) = default;
    PoolAllocator&amp; operator=(const PoolAllocator&amp; other) = default;
    PoolAllocator&amp; operator=(PoolAllocator&amp;&amp; other) = default;

    T* allocate(size_t n) {
        if (n &gt; 1) {
            return static_cast&lt;T*&gt;(malloc(sizeof(T) * n));
        }

        return static_cast&lt;T*&gt;(pool_-&gt;allocate());
    }

    void deallocate(T* ptr, size_t n) {
        if (n &gt; 1) {
            free(ptr);
            return;
        }

        pool_-&gt;deallocate(ptr);
    }
};
</code></pre>
<p>Our allocator class looks very similar as our previous one. The constructor creates a <code>Pool</code> instance which will be used to allocate and deallocate memory. The <code>allocate()</code> and <code>deallocate()</code> methods simply pass the call onto the <code>pool_</code> instance. Note that our allocator only supports individual allocations: if n &gt; 1, we simply use the standard allocator via <code>malloc()</code> and <code>free()</code>.</p>
<p>We must also provide a <code>struct rebind</code> because our class has more than one template parameter and a rebind copy constructor which just passes the call down to the <code>pool_</code> instance. We also need to provide default copy/move constructors and assignment operators.</p>
<p>Our benchmark in this case will measure the time it takes to add 1 million integers to a <code>std::list</code> (see the code <a href="https://github.com/thealexcons/cpp-allocators/blob/master/pool_allocator.cc">here</a>). We compare the standard allocator, a pool allocator with no reserved blocks in-advance, one with 100 reserved blocks and another with 1000 reserved blocks:</p>
<pre><code>std::allocator&lt;int&gt;            mean: 21611 &#x3BC;s
PoolAllocator&lt;int&gt;             mean: 12885 &#x3BC;s
PoolAllocator&lt;int, 4096, 100&gt;  mean: 5718 &#x3BC;s
PoolAllocator&lt;int, 4096, 1000&gt; mean: 5686 &#x3BC;s
</code></pre>
<p>Even without reserving blocks in advance, we get around x2 speed up! And as expected, if we reserve blocks in advance, we get a x3.85 speed up.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Pool allocators are so useful that they were <a href="https://en.cppreference.com/w/cpp/memory/synchronized_pool_resource">introduced</a> into the standard library in C++17. You can access them in the <code>&lt;pmr&gt;</code> header (<em>polymorphic memory resource</em>) and use them nicely with STL containers.</p>
<!--kg-card-end: markdown--><h2 id="example-3-a-huge-page-allocator">Example 3: A <em>huge page</em> allocator</h2><p>Our final example is just to show off how you can write highly-tailored allocators using a specific feature of Linux: transparent huge pages.</p><p>Typically, the OS will allocate memory in fixed-size pages of 4 kB. However, if we have a very large in-memory data structure spanning multiple pages which may be randomly accessed (such as a hash table), we are likely going to suffer from a high number of TLB misses during virtual address translation. The OS can reduce this miss rate by allocating large-sized pages, known as <strong>huge pages</strong>. These are typically 2 MB, but can be even larger.</p><!--kg-card-begin: markdown--><p>A <em>transparent</em> huge page is one that was promoted automatically by the OS from a regular page into a huge page, and is a Linux-specific feature. We can enable THP support for a memory range using the <code>madvise()</code> system call, however Linux does not guarantee that a HP will be allocated. We can use <code>posix_memalign</code> to hint at the kernel that we really want to allocate a huge page. Let&apos;s see the code:</p>
<pre><code class="language-c++">template &lt;typename T, size_t HugePageSize = 1 &lt;&lt; 21&gt; 
class THPAllocator {
public:
    using is_always_equal = std::true_type;
    using value_type = T;

    template &lt;typename U&gt;
    struct rebind {
        using other = THPAllocator&lt;U, HugePageSize&gt;;
    };

    THPAllocator() = default;

    template &lt;class U&gt;
    constexpr THPAllocator(const THPAllocator&lt;U&gt;&amp; other) {
        (void) other;
    }

    T *allocate(size_t n) {
        if (n &gt; std::numeric_limits&lt;std::size_t&gt;::max() / sizeof(T)) {
            throw std::bad_alloc();
        }
        const auto total_size = n * sizeof(T);
        void *p = nullptr;
        if (posix_memalign(&amp;p, HugePageSize, total_size) != 0) {
            throw std::bad_alloc();
        }
        
        madvise(p, total_size, MADV_HUGEPAGE);
        if (p == nullptr) {
            throw std::bad_alloc();
        }

        return static_cast&lt;T *&gt;(p);
    }

    void deallocate(T *p, size_t n) { 
        (void) n;
        free(p); 
    }
};
</code></pre>
<p>Our <a href="https://github.com/thealexcons/cpp-allocators/blob/master/huge_page_allocator.cc">benchmark</a> is very simple: we try to add 8 MB worth of integers to a vector and see how long it takes. We obtain:</p>
<pre><code>std::allocator mean: 4414 &#x3BC;s
THPAllocator   mean: 2584 &#x3BC;s
</code></pre>
<p>It is also worth noting that requesting the kernel to promote a regular page into a huge page may cause latency spikes, because the kernel needs to update the page tables accordingly. Also, the kernel may decide to compact unused pages in order to create a huge page on demand, which can also lead to latency spikes. If you try running the benchmark with <code>iterations = 1</code>, you will see the large variance.</p>
<!--kg-card-end: markdown--><h2 id="closing-thoughts">Closing thoughts</h2><p>We have seen how C++ easily lets us implement custom memory allocators for different applications and use cases. The C++ allocator API can go into much more detail and I am not an expert on memory allocators, this post is just an example highlighting the benefits of memory allocators and a simple demo on how to implement them.</p>]]></content:encoded></item><item><title><![CDATA[Implementing an optimised SPSC queue for low-latency message passing]]></title><description><![CDATA[We look at how to implement a fully optimised SPSC queue, for high-throughput message passing]]></description><link>http://localhost:2368/spsc-queue/</link><guid isPermaLink="false">61ca4526b67ce12190e4f7d3</guid><dc:creator><![CDATA[Alex Constantin-Gomez]]></dc:creator><pubDate>Tue, 28 Dec 2021 12:31:35 GMT</pubDate><content:encoded><![CDATA[<p></p><h2 id="background">Background</h2><p>A single-producer single-consumer queue is a FIFO buffer that acts as a message passing mechanism between two threads (the producer and the consumer). This can be especially useful when a thread produces (or receives) data which should then be passed onto another thread for processing. </p><p>An example usage of such queue would be for asynchronous logging: if we have a latency-sensitive section of code and we wish to log something, we should pass it to another background thread which deals with string formatting and I/O. Like this, the only cost incurred in the critical path is just push the contents of the log onto the queue.</p><p>Naturally, in a multi-threaded environment, we may decide to have multiple producers and multiple consumers passing data around. However, this requires a more careful implementation to avoid data races and often, using locks which can lead to lower performance (due to context switches). Although implementations without using mutexes <a href="https://www.1024cores.net/home/lock-free-algorithms/queues/bounded-mpmc-queue">exist</a> (via atomic RMWs), they are not technically lock-free nor trivial to implement.</p><p>On the other hand, if we have exactly 2 threads (one writer and one reader) sharing the queue, it is possible to implement a wait-free SPSC queue using only atomic loads and stores (no RMW loops!).</p><blockquote>A wait-free algorithm means that all the threads in the system make progress regardless of contention and the operations are executed in a finite number of steps.</blockquote><p>This is largely why SPSC queues are commonly found in high-throughput multi-threaad systems: they are very fast and fairly easy to implement.</p><h2 id="v1-a-simple-wait-free-spsc-queue">V1: A simple wait-free SPSC queue</h2><!--kg-card-begin: markdown--><p>We will implement the queue as a circular ring buffer using <code>std::vector</code> as the underlying container. We also need to keep track of the <code>head</code> and <code>tail</code> fields, which are atomically updated. The queue will have a bounded capacity, which is specified on construction.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code class="language-c++">template &lt;typename T&gt;
class SpscQueue {
private:
    std::vector&lt;T&gt; buffer_;
    std::atomic&lt;size_t&gt; head_;
    std::atomic&lt;size_t&gt; tail_;
    
public:
    SpscQueue(size_t cap) : buffer_(cap + 1), head_(0), tail_(0) {
        assert(cap + 1 &gt; 0);      // prevent overflow
    }
    
    ...
}
</code></pre>
<!--kg-card-end: markdown--><p>Notice that we are actually providing the capacity plus one to the vector. The reason for this is that we require one item in the queue to distinguish when the queue is empty or full, such that our empty/full conditions are given by:</p><!--kg-card-begin: markdown--><ul>
<li>Is empty? <code>head_ == tail_</code></li>
<li>Is full?  <code>head_ == (tail_ + 1) % queue_size</code></li>
</ul>
<!--kg-card-end: markdown--><p>Given this information, we can implement our enqueue and dequeue methods. The code is given first and then explained:</p><!--kg-card-begin: markdown--><pre><code class="language-c++">bool enqueue(const T&amp; item) {
    const size_t t = tail_.load(std::memory_order_relaxed);
    const size_t next_t = (t + 1) % buffer_.size();

    if (next_t == head_.load(std::memory_order_acquire)) {
        return false;
    }

    buffer_[t] = item;
    tail_.store(next_t, std::memory_order_release);
    return true;
}

bool dequeue(T&amp; item) {
    const size_t h = head_.load(std::memory_order_relaxed);

    if (h == tail_.load(std::memory_order_acquire)) {
        return false;
    }

    item = buffer_[h];
    const size_t next_h = (h + 1) % buffer_.size();
    head_.store(next_h, std::memory_order_release);
    return true;
}

</code></pre>
<!--kg-card-end: markdown--><p>As you can see, there are no atomic RMW loops or blocking operations, so both methods are guaranteed to be wait-free and lock-free.</p><!--kg-card-begin: markdown--><p>Let&apos;s start with <code>enqueue()</code>. We first check that the queue is not full by comparing the next possible tail index with the current head index. We use <code>std::memory_order_acquire</code> to synchronise with possible updates to the <code>head_</code> field by the consumer thread calling <code>dequeue()</code>.<br>
We then store the item into the queue at the current tail and update the <code>tail_</code> field atomically to point to the next location. Because we want to synchronise with the <code>dequeue()</code> method, we use <code>std::memory_order_release</code> so that our local changes are visible to the other thread. This also happens to be our linearisation point, ie the point at which the <code>enqueue()</code> method appears to take effect.</p>
<p>The reasoning for the <code>dequeue()</code> method is symmetric, but instead we check that the queue is not empty and update the  <code>head_</code> field atomically.</p>
<!--kg-card-end: markdown--><p>Initially, I tried implementing the queue using only sequentially consistent atomic operations, however the performance is much worse (nearly x4 lower throughput). I recommend you benchmark an SC-only version of the queue to see how much slower it can be (due to memory barriers).</p><p>We can now benchmark this unoptimised implementation with a simple program consisting of two threads passing 100 million integers from one to the other, with a queue size of 100k. We measure how long it takes for the consumer thread to read all of the items.</p><p>We also pin the consumer and producer threads to different physical cores in our benchmark program. We will see why has makes an important point in the next section.</p><!--kg-card-begin: markdown--><p>We compile the benchmark with O3 optimisations and <code>-march=native</code>, and we obtain the following results (run over 10 iterations):</p>
<pre><code>Mean:   46,247,967 elems/s
Median: 46,477,813 elems/s
Min:    44,271,890 elems/s
Max:    46,672,184 elems/s
</code></pre>
<p>We are processing around 46 million elements per second, which is quite good, but we can do even better.</p>
<!--kg-card-end: markdown--><h2 id="v2-eliminating-false-sharing">V2: Eliminating false sharing</h2><p>Going back to our class definition, we see that our head and tail fields are defined contiguously in memory. This can lead to <strong>false sharing</strong>, where thread 1 modifies one of the fields and this invalidates thread 2&apos;s cache line because both fields lie on the same cache line. This forces thread 2 to go to main memory to fetch the other field even though thread 1 did not change it.</p><p>We can easily fix this by adding enough padding between the two fields such that they end up falling on different cache lines. On most modern processors, an L1 cache line is typically 64 bytes, so we can align both fields at 64 bytes each. We can do this as follows in C++:</p><!--kg-card-begin: markdown--><pre><code class="language-c++">class SpscQueue {
private:
    std::vector&lt;T&gt; buffer_;
    alignas(64) std::atomic&lt;size_t&gt; head_;
    alignas(64) std::atomic&lt;size_t&gt; tail_;
    
...
}
</code></pre>
<p>Our queue should no longer suffer from false sharing! Our new results show a slight improvement:</p>
<pre><code>Mean:   49,357,127 elems/s
Median: 49,245,343 elems/s
Min:    47,540,700 elems/s
Max:    50,012,641 elems/s
</code></pre>
<!--kg-card-end: markdown--><h2 id="v3-a-cache-optimised-queue">V3: A cache-optimised queue</h2><p>The final but substantial optimisation aims to improve cache usage and reduce cache misses (inspired by <a href="https://github.com/rigtorp/SPSCQueue">this</a>).</p><!--kg-card-begin: markdown--><p>Consider the case when the consumer calls <code>dequeue()</code> to read an item:</p>
<ol>
<li>The <code>head_</code> needs to be updated, so that cache line is loaded into the L1 cache in an <strong>exclusive state</strong>.</li>
<li>The <code>tail_</code> needs to be read to check that the queue is not empty, so its cache line is loaded into the L1 cache in a <strong>shared state</strong>.</li>
</ol>
<p>Now, assume the producer calls <code>enqueue()</code> to push a new item:<br>
3. The <code>head_</code> needs to be read (to check that it is not full), so it causes the consumer&apos;s cache line containing <code>head_</code> to transition into a shared state.<br>
4. This causes cache coherency traffic, as the consumer needs to bring back the <code>head_</code> cache line into an exclusive state.</p>
<p>A symmetric situation occurs the other way round, meaning that in the worst case, there will be <strong>one cache line transition from a shared state to an exclusive state</strong> for every read and write. In the <a href="https://en.wikipedia.org/wiki/MESI_protocol">MESI cache coherency protocol</a>, these transitions are considered cache misses and produce bus traffic.</p>
<p>To reduce bus traffic, the producer and consumer threads will each have their own cached copies of the head and tail indices which can be used to avoid having to always load the <code>head_</code> or <code>tail_</code> when checking if the queue is empty or full.</p>
<p>Essentially, when the consumer first observes that N items are available to read, it caches this information and the N-1 subsequent reads won&#x2019;t need to read the <code>tail_</code>. Similarly when the producer first observes that N items are available for writing, it caches this information and the N-1 subsequent writes won&#x2019;t need to read the <code>head_</code>.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Our cache-friendly implementation looks like this now:</p>
<pre><code class="language-c++">bool enqueue(const T&amp; item) {
    const size_t t = tail_.load(std::memory_order_relaxed);
    const size_t next_t = (t + 1) % buffer_.size();

    // Use the cached head first instead of loading the actual head from memory.
    // If they are equal, then we know that the queue may be full, so only then load
    // the actual value of head to check if currently full.
    if (next_t == head_cached_) {
        head_cached_ = head_.load(std::memory_order_acquire);
        if (next_t == head_cached_) {
            return false;
        }
    }

    buffer_[t] = item;
    tail_.store(next_t, std::memory_order_release);
    return true;
}

bool dequeue(T&amp; item) {
    const size_t h = head_.load(std::memory_order_relaxed);

    // Use the cached tail first instead of loading the actual tail from memory.
    // If they are equal, then we know that the queue may be empty, so only then load
    // the actual value of tail to check if currently full.
    if (h == tail_cached_) {
        tail_cached_ = tail_.load(std::memory_order_acquire);
        if (h == tail_cached_) {
            return false;
        }
    }

    item = buffer_[h];
    const size_t next_h = (h + 1) % buffer_.size();
    head_.store(next_h, std::memory_order_release);
    return true;
}

</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Compiling and running our benchmark again with our new queue implementation, we obtain:</p>
<pre><code>Mean:   101,138,856 elems/s
Median: 101,566,018 elems/s
Min:    97,563,021 elems/s
Max:    102,558,883 elems/s
</code></pre>
<p>We have more than doubled our initial throughput, reaching 100 million items per second. A fantastic improvement! Obviously, the results shown throughout this post are dependent on your hardware, but we have clearly optimised the queue, starting from a simple implementation and finishing with a cache-efficient queue.</p>
<!--kg-card-end: markdown--><p>The code for the queue and the benchmark can be found <a href="https://github.com/thealexcons/spsc-queue">here</a>.</p><h2 id="closing-thoughts">Closing thoughts</h2><p>We have gone from considering a naive SPSC queue using sequentially consistent operations, introducing weaker memory orderings and improving cache efficiency by understanding the hardware, to obtain a high-throughput low-latency message passing queue.</p><p>I find this process of benchmarking and tuning very interesting, especially when seeing that your initial theories drawn up on paper can be translated into big performance boosts in practice.</p><p>There are further micro-optimisations you can try and profile, such as ensuring the buffer size is a power of two (in order to avoid performing integer division), using huge pages to reduce TLB misses during virtual address translation, etc.</p>]]></content:encoded></item></channel></rss>