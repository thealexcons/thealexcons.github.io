<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alex Constantin-Gomez</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="/styles.css">

</head>
<body>
    <header>
        <h1>Alex Constantin-Gomez</h1>
        <nav>
            <a href="/index.html">home</a>
            <a href="/articles.html">articles</a>
            <a href="/about.html">about</a>
        </nav>
    </header>

    <main>
        <article>
            <div class="article-header">
                <h1 class="article-title">Accelerating data center communication patterns with eBPF</h1>
                <div class="article-meta">posted: 27.06.2023</div>
            </div>

            <div class="article-content">
                <p>I explore the use of eBPF (extended Berkeley Packet Filter) as a solution to improve the communication efficiency in highly-distributed data center environments, focusing on minimising the
                    overhead caused by user-kernel context switches and excessive traversals of the kernel networking
                    stack. By leveraging eBPF, which allows user-defined code execution inside the Linux kernel, it
                    becomes possible to move logic from the application in user-space into the kernel and achieve significant performance benefits.<br><br>
                    The work presented focuses on accelerating scatter-gather workloads, which involve extensive communication between a coordinator machine and multiple worker nodes. I first explore the feasibility of using eBPF to accelerate this communication pattern, and then propose an eBPF-enabled
                    scatter-gather network primitive called sgbpf available as a library for network applications.<br><br>
                    Our experiments show that sgbpf outperforms the standard Linux-native I/O APIs (including
                    state-of-the-art interfaces such as epoll and io_uring) by at least 42% both in terms of latency
                    and throughput for fan-outs of all sizes, varying from as low as 10 nodes all the way up to over
                    1000 workers</p>

                <embed src="/meng-project-slides.pdf" width="100%" height="500" type="application/pdf">

                <p>Code available <a href="https://github.com/thealexcons/sgbpf">here</a>.</p>
            </div>

        </article>
    </main>

    <!-- Prism.js scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html>